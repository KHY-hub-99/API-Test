import os
import multiprocessing

available_cores = multiprocessing.cpu_count()
JAVA_PARALLELISM = 2
if available_cores > JAVA_PARALLELISM:
    JAVA_PARALLELISM = JAVA_PARALLELISM
else:
    JAVA_PARALLELISM = available_cores
print(f"âš™ï¸  ì„¤ì •ëœ ì‚¬ìš© ì½”ì–´ ìˆ˜: {JAVA_PARALLELISM}ê°œ")
os.environ["JAVA_HOME"] = r"C:\Program Files\Java\jdk-21.0.10"
os.environ["JAVA_OPTS"] = f"-Xmx8G -Djava.util.concurrent.ForkJoinPool.common.parallelism={JAVA_PARALLELISM}"

from google import genai
import zipfile
import json
import pandas as pd
import geopandas as gpd
import math
from datetime import datetime, timedelta
from dotenv import load_dotenv
from ortools.constraint_solver import routing_enums_pb2, pywrapcp
import time
import re
from r5py import TransportNetwork, TravelTimeMatrix, DetailedItineraries, TransportMode
import pickle
from concurrent.futures import ThreadPoolExecutor

# ============================================================
# 1. í™˜ê²½ ì„¤ì • ë° ì „ì—­ ìƒìˆ˜
# ============================================================ 
# API í‚¤ ì„¤ì •
load_dotenv()
API_KEY = os.getenv("API_KEY")
client = genai.Client(api_key=API_KEY)

# ìºì‹œ ì €ì¥ì†Œ
DETAILED_PATH_CACHE = {}

# í´ë°±(ì¢Œí‘œ ì—†ëŠ” ê²½ìš°) ì´ë™ ì‹œê°„ ì„¤ì •(ë¶„)
FALLBACK_MOVE_MIN = 30

# ë„ë³´ ì´ë™ ì œí•œ (km -> ë¶„ í™˜ì‚° ê¸°ì¤€ ë“±)
WALK_ONLY_THRESHOLD_MIN = 12   
WALK_ONLY_THRESHOLD_MAX = 18   

MAX_TRANSFERS = 3
MAX_TRAVEL_TIME_MIN = 90

# ì‹œê°„ ìœˆë„ìš° ì„¤ì •
LUNCH_WINDOW = ("11:20", "13:20")
DINNER_WINDOW = ("17:40", "19:30")

# ì¥ì†Œë³„ ì²´ë¥˜ ì‹œê°„
stay_time_map = {
    "ê´€ê´‘ì§€": 90, "ì¹´í˜": 50, "ì‹ë‹¹": 70, 
    "ë°•ë¬¼ê´€": 120, "ê³µì›": 60, "ì‹œì¥": 80, "ìˆ™ë°•": 0
}

# ë°ì´í„° íŒŒì¼ ê²½ë¡œ
osm_file = "./data/seoul_osm_v.pbf"
gtfs_files = ["./data/seoul_area_gtfs.zip"]

# ============================================================
# 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================
def parse_time(t):
    return datetime.strptime(t, "%H:%M")

def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # km
    phi1, phi2 = math.radians(lat1), math.radians(lat2)
    dphi = math.radians(lat2 - lat1)
    dlambda = math.radians(lon2 - lon1)
    a = math.sin(dphi / 2) ** 2 + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2) ** 2
    return 2 * R * math.atan2(math.sqrt(a), math.sqrt(1 - a))

def approx_walk_minutes(start, end):
    # start/endê°€ ì¢Œí‘œ ì—†ìŒ(None)ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „ ì²˜ë¦¬
    if not start or not end or start.get("lat") is None or end.get("lat") is None:
        return FALLBACK_MOVE_MIN
    dist_km = haversine(start["lat"], start["lng"], end["lat"], end["lng"])
    return dist_km * 15

def dynamic_walk_threshold(dist_km):
    if dist_km < 0.6: return WALK_ONLY_THRESHOLD_MAX
    elif dist_km < 1.2: return 15
    else: return WALK_ONLY_THRESHOLD_MIN

def travel_minutes(p1, p2):
    # ì¢Œí‘œê°€ ì—†ìœ¼ë©´ 0ì„ ë°˜í™˜(ìƒìœ„ ë¡œì§ì—ì„œ ê³ ì •ì¼ì • ë³´ì •ìœ¼ë¡œ ìµœì†Œ ì‹œê°„ ì ìš©ë¨)
    if p1 is None or p2 is None or p1.get("lat") is None or p2.get("lat") is None: return 0
    dist = haversine(p1["lat"], p1["lng"], p2["lat"], p2["lng"])
    return int(dist / 30 * 60)

def get_fixed_events_for_day(fixed_events, target_date):
    return [e for e in fixed_events if e["date"] == target_date]

def duration_to_minutes(val):
    if val is None or pd.isna(val): return 0
    if hasattr(val, "total_seconds"): return int(val.total_seconds() / 60)
    if isinstance(val, str) and "day" in val:
        try: return int(pd.to_timedelta(val).total_seconds() / 60)
        except: return 0
    try: return int(float(val))
    except: return 0

def extract_json(text):
    if not text:
        raise ValueError("Gemini ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    text = text.strip()
    if text.startswith("```"):
        text = text.split("```")[1]
        if text.startswith("json"):
            text = text[4:]
    start = text.find("{")
    end = text.rfind("}") + 1
    if start == -1 or end == -1:
        raise ValueError("JSON íŒŒì‹± ì‹¤íŒ¨:\n" + text)
    return json.loads(text[start:end])

# ============================================================
# 3. êµí†µ ë°ì´í„° ë¡œë“œ (GTFS & OSM)
# ============================================================

# 3-1. TransportNetwork (ê¸°ì¡´ ìœ ì§€)
pickle_path = "./data/seoul_tn_cached.pkl"
if os.path.exists(pickle_path):
    print(f"ğŸ“¦ TransportNetwork ë¡œë“œ ì¤‘...")
    transport_network = TransportNetwork.__new__(TransportNetwork)
    transport_network._transport_network = TransportNetwork._load_pickled_transport_network(self=TransportNetwork, path=pickle_path)
else:
    print("ğŸš€ TransportNetwork ìƒì„± ì¤‘...")
    transport_network = TransportNetwork(osm_file, gtfs_files)
    transport_network._save_pickled_transport_network(path=pickle_path, transport_network=transport_network)

# 3-2 & 3-3. ë©”íƒ€ë°ì´í„°(Stop/Route) ê³ ì† ë¡œë“œ (Pickle ì ìš©)
meta_cache_path = "./data/metadata_cache.pkl"

if os.path.exists(meta_cache_path):
    print("âš¡ ë©”íƒ€ë°ì´í„° ìºì‹œ ë¡œë“œ ì¤‘...")
    with open(meta_cache_path, "rb") as f:
        meta_data = pickle.load(f)
        STOP_ID_TO_NAME = meta_data["stops"]
        ROUTE_ID_TO_NAME = meta_data["routes"]
        STOP_ROUTE_MAP = meta_data["stop_route_map"]
else:
    print("ğŸ¢ ë©”íƒ€ë°ì´í„° ìƒì„± ì¤‘ (ìµœì´ˆ 1íšŒë§Œ ëŠë¦¼)...")
    # Stops
    with zipfile.ZipFile(gtfs_files[0]) as z:
        with z.open("stops.txt") as f:
            stops_df = pd.read_csv(f, dtype={'stop_id': str})
    STOP_ID_TO_NAME = {str(row['stop_id']).strip(): str(row['stop_name']).strip() for _, row in stops_df.iterrows()}
    
    # Routes
    with zipfile.ZipFile(gtfs_files[0]) as z:
        with z.open("routes.txt") as f:
            routes_df = pd.read_csv(f)
    ROUTE_ID_TO_NAME = dict(zip(routes_df["route_id"].astype(str), routes_df["route_short_name"].astype(str)))
    
    # Stop-Route Map
    try:
        with zipfile.ZipFile(gtfs_files[0]) as z:
            with z.open("trips.txt") as f:
                trips = pd.read_csv(f, usecols=["route_id", "trip_id"])
            with z.open("stop_times.txt") as f:
                stop_times = pd.read_csv(f, usecols=["trip_id", "stop_id"], dtype={"stop_id": str})
        merged = stop_times.merge(trips, on="trip_id")[["stop_id", "route_id"]].drop_duplicates()
        grouped = merged.groupby("stop_id")["route_id"].apply(set)
        STOP_ROUTE_MAP = grouped.to_dict()
    except Exception as e:
        print(f"âš ï¸ ë§¤í•‘ ì‹¤íŒ¨: {e}")
        STOP_ROUTE_MAP = {}

    # ìºì‹œ ì €ì¥
    with open(meta_cache_path, "wb") as f:
        pickle.dump({
            "stops": STOP_ID_TO_NAME,
            "routes": ROUTE_ID_TO_NAME,
            "stop_route_map": STOP_ROUTE_MAP
        }, f)

# Helper í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ìœ ì§€)
def get_stop_name(stop_id):
    if pd.isna(stop_id): return None
    safe_id = str(stop_id).strip()
    try: safe_id = str(int(float(stop_id))).strip()
    except: pass
    name = STOP_ID_TO_NAME.get(safe_id)
    if not name and len(safe_id) < 5: name = STOP_ID_TO_NAME.get(safe_id.zfill(5))
    return name

def get_route_name(route_id):
    if pd.isna(route_id): return None
    try: safe_id = str(int(float(route_id)))
    except: safe_id = str(route_id)
    return ROUTE_ID_TO_NAME.get(safe_id)

# ============================================================
# 4. ê²½ë¡œ ê³„ì‚° ë° ìƒì„¸í™” (r5py) - ì•ˆì „ì„± ë³´ê°•
# ============================================================

def get_r5py_matrix(nodes, departure_time):
    valid_nodes = [n for n in nodes if n.get("lat") is not None]
    if len(valid_nodes) < 2: return {}

    gdf = gpd.GeoDataFrame(
        valid_nodes,
        geometry=gpd.points_from_xy([n['lng'] for n in valid_nodes], [n['lat'] for n in valid_nodes]),
        crs="EPSG:4326"
    )

    try:
        matrix = TravelTimeMatrix(
            transport_network, origins=gdf, destinations=gdf, departure=departure_time,
            transport_modes=[TransportMode.WALK, TransportMode.TRANSIT]
        )
        r5_travel_times = {}
        for row in matrix.itertuples():
            if not pd.isna(row.travel_time):
                r5_travel_times[(int(row.from_id), int(row.to_id))] = int(row.travel_time)
        return r5_travel_times
    except Exception as e:
        print(f"âš ï¸ í–‰ë ¬ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return {}


def make_cache_key(start_node, end_node, departure_time):
    # ìˆ˜ì • ì „: IDë§Œ ì‚¬ìš© -> ë‚ ì§œê°€ ë‹¬ë¼ë„ IDê°€ ê°™ìœ¼ë©´ ì¶©ëŒ ë°œìƒ
    # return (s_id, e_id, int(departure_time.hour))

    # ìˆ˜ì • í›„: 'ì¥ì†Œ ì´ë¦„'ì„ í¬í•¨í•˜ì—¬ ìœ ì¼ì„± ë³´ì¥
    s_name = start_node.get("name", str(start_node.get("id")))
    e_name = end_node.get("name", str(end_node.get("id")))
    
    # ê³ ì • ì¼ì • ë“±ì˜ ê²½ìš° ì¢Œí‘œê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ë¦„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ë¶„
    return (s_name, e_name, int(departure_time.hour))


def get_all_detailed_paths(trip_legs, departure_time):
    """
    trip_legs: [(start_node, end_node), ...]
    ì•ˆì „ ì¡°ì¹˜:
      - ì¢Œí‘œê°€ ì—†ëŠ” ë…¸ë“œ(ì˜ˆ: ê³ ì •ì¼ì •)ëŠ” r5py ìš”ì²­ ëŒ€ìƒì—ì„œ ì œì™¸
      - ì¢Œí‘œ ì—†ëŠ” êµ¬ê°„ì— ëŒ€í•´ì„  í´ë°± ê²½ë¡œ(fallback)ë¥¼ ë§Œë“¤ì–´ path_mapì— ë„£ìŒ
    """
    if not trip_legs: return {}
    path_map = {}
    origins_list, dests_list = [], []

    # 1) ìš”ì²­í•  (ì¢Œí‘œ ìˆëŠ”) ìŒë§Œ ìˆ˜ì§‘í•˜ê³ , ì¢Œí‘œ ì—†ëŠ” ìŒì€ í´ë°±ìœ¼ë¡œ ì²˜ë¦¬
    for start_node, end_node in trip_legs:
        if start_node['id'] == end_node['id']: continue

        cache_key = make_cache_key(start_node, end_node, departure_time)
        if cache_key in DETAILED_PATH_CACHE:
            path_map[(int(start_node['id']), int(end_node['id']))] = DETAILED_PATH_CACHE[cache_key]
            continue

        # ì¢Œí‘œê°€ ì—†ìœ¼ë©´ r5 ìš”ì²­ì„ ë§Œë“¤ì§€ ì•Šê³  í´ë°±ìœ¼ë¡œ ì±„ì›€
        if start_node.get('lat') is None or end_node.get('lat') is None:
            fallback_entry = {"fastest": [f"ì´ë™(ì¢Œí‘œì—†ìŒ) : {FALLBACK_MOVE_MIN}ë¶„"], "min_transfer": [f"ì´ë™(ì¢Œí‘œì—†ìŒ) : {FALLBACK_MOVE_MIN}ë¶„"]}
            DETAILED_PATH_CACHE[cache_key] = fallback_entry
            path_map[(int(start_node['id']), int(end_node['id']))] = fallback_entry
            continue

        # ì¢Œí‘œê°€ ëª¨ë‘ ìˆìœ¼ë©´ r5 ìš”ì²­ ëŒ€ìƒì— ì¶”ê°€
        origins_list.append(start_node)
        dests_list.append(end_node)

    # 2) ì¢Œí‘œ ìˆëŠ” ìŒë§Œ r5pyë¡œ ìƒì„¸ ê²½ë¡œ ìš”ì²­
    if origins_list:
        origins_gdf = gpd.GeoDataFrame(origins_list, geometry=gpd.points_from_xy([n['lng'] for n in origins_list], [n['lat'] for n in origins_list]), crs="EPSG:4326")
        origins_gdf["id"] = [n["id"] for n in origins_list]
        dests_gdf = gpd.GeoDataFrame(dests_list, geometry=gpd.points_from_xy([n['lng'] for n in dests_list], [n['lat'] for n in dests_list]), crs="EPSG:4326")
        dests_gdf["id"] = [n["id"] for n in dests_list]

        try:
            computer = DetailedItineraries(
                transport_network, origins=origins_gdf, destinations=dests_gdf, departure=departure_time,
                transport_modes=[TransportMode.WALK, TransportMode.TRANSIT],
                max_public_transport_rides=MAX_TRANSFERS, max_time=timedelta(minutes=MAX_TRAVEL_TIME_MIN)
            )
        except Exception as e:
            print(f"âš ï¸ DetailedItineraries í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            computer = None

        if computer is not None and not computer.empty:
            mode_col = 'transport_mode' if 'transport_mode' in computer.columns else 'mode'

            def get_val(row, candidates, default=None):
                for c in candidates:
                    if c in row.index and pd.notna(row[c]): return str(row[c]).strip()
                return default

            def parse_route_to_segments(route_df):
                segs = []
                for _, leg in route_df.iterrows():
                    raw_mode = str(leg[mode_col]).upper() if mode_col in leg.index else ''

                    # ì‹œê°„ íŒŒì‹±
                    ride_time = max(1, duration_to_minutes(get_val(leg, ['travel_time', 'duration'], 0)))
                    wait_time = duration_to_minutes(get_val(leg, ['wait_time', 'wait'], 0))

                    if wait_time > 0:
                        segs.append(f"ëŒ€ê¸° : {wait_time}ë¶„")

                    if 'WALK' in raw_mode:
                        segs.append(f"ë„ë³´ : {ride_time}ë¶„")
                        continue

                    f_id, t_id = str(get_val(leg, ['start_stop_id', 'from_stop_id'])), str(get_val(leg, ['end_stop_id', 'to_stop_id']))
                    f_stop, t_stop = get_stop_name(f_id) or "ì •ë¥˜ì¥", get_stop_name(t_id) or "ì •ë¥˜ì¥"
                    c_rid = str(get_val(leg, ['route_id']))
                    mode_lbl = "ì§€í•˜ì² " if any(x in raw_mode for x in ['SUBWAY', 'RAIL', 'METRO']) else "ë²„ìŠ¤"

                    if mode_lbl == "ë²„ìŠ¤" and STOP_ROUTE_MAP:
                        common = STOP_ROUTE_MAP.get(f_id, set()).intersection(STOP_ROUTE_MAP.get(t_id, set()))
                        common.add(c_rid)
                        b_names = sorted([n for n in [get_route_name(rid) for rid in common] if n])
                        r_str = ", ".join(b_names) if b_names else (get_route_name(c_rid) or 'ëŒ€ì¤‘êµí†µ')
                    else:
                        r_str = get_route_name(c_rid) or 'ëŒ€ì¤‘êµí†µ'

                    segs.append(f"[{mode_lbl}][{r_str}] : {f_stop} â†’ {t_stop} : {ride_time}ë¶„")

                return segs

            # 3) ê·¸ë£¹ë³„ ì˜µì…˜ ë¶„ì„
            for (from_id, to_id), group in computer.groupby(['from_id', 'to_id']):
                options_data = []
                for _, opt in group.groupby("option"):
                    t_min = sum(max(1, duration_to_minutes(get_val(leg, ['travel_time', 'duration'], 0))) for _, leg in opt.iterrows())
                    t_count = sum(1 for _, leg in opt.iterrows() if 'WALK' not in str(leg[mode_col]).upper())
                    options_data.append({"route": opt, "time": t_min, "transfers": t_count})

                if not options_data:
                    continue

                fastest_opt = min(options_data, key=lambda x: (x['time'], x['transfers']))
                result_entry = {"fastest": parse_route_to_segments(fastest_opt['route'])}

                walk_opts = [o for o in options_data if o['transfers'] == 0]
                best_walk = min(walk_opts, key=lambda x: x['time']) if walk_opts else None

                transit_opts = [o for o in options_data if o['transfers'] > 0]
                transit_opts.sort(key=lambda x: (x['transfers'], x['time']))
                best_transit = transit_opts[0] if transit_opts else None

                winner_opt = None
                if best_walk and best_transit:
                    if best_walk['time'] <= best_transit['time']:
                        winner_opt = best_walk
                    else:
                        winner_opt = best_transit
                elif best_transit:
                    winner_opt = best_transit
                else:
                    winner_opt = best_walk

                if winner_opt:
                    result_entry["min_transfer"] = parse_route_to_segments(winner_opt['route'])
                else:
                    # ë“œë¬¼ê²Œ ì˜µì…˜ì´ ë¹„ì–´ìˆìœ¼ë©´ í´ë°± ì ìš©
                    result_entry["min_transfer"] = [f"ë„ë³´ : {FALLBACK_MOVE_MIN}ë¶„"]

                cache_key = (int(from_id), int(to_id), int(departure_time.hour))
                DETAILED_PATH_CACHE[cache_key] = result_entry
                path_map[(int(from_id), int(to_id))] = result_entry

    return path_map

# ============================================================
# 5. ë…¸ë“œ ë¹Œë” & ìµœì í™” (OR-Tools)
# ============================================================

def build_fixed_nodes(fixed_events, day_start_dt):
    nodes = []
    BUFFER = 15
    for idx, event in enumerate(fixed_events):
        event_start = parse_time(event["start"]) if event.get("start") else day_start_dt
        event_end = parse_time(event["end"]) if event.get("end") else day_start_dt
        orig_start_min = int((event_start - day_start_dt).total_seconds() / 60)
        orig_end_min = int((event_end - day_start_dt).total_seconds() / 60)

        raw_start_min = orig_start_min - BUFFER
        buffered_start_min = max(0, raw_start_min)
        final_stay = (orig_end_min - orig_start_min) + (orig_start_min - buffered_start_min) + BUFFER

        # ê³ ì •ì¼ì •ì€ ì¢Œí‘œê°€ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ lat/lngëŠ” Noneìœ¼ë¡œ ë‘ 
        nodes.append({
            "name": event.get("title", "ê³ ì •ì¼ì •"), "category": "ê³ ì •ì¼ì •", "lat": None, "lng": None,
            "stay": final_stay, "type": "fixed", "window": (buffered_start_min, buffered_start_min + 10),
            "orig_time_str": f"{event.get('start','00:00')} - {event.get('end','00:00')}"
        })
    return nodes

def build_nodes(places, restaurants, fixed_events, day_start_dt):
    nodes = []
    first_place = places[0] if places else {"lat": 37.5665, "lng": 126.9780}
    nodes.append({"name": "ì‹œì‘ì ", "category": "ì¶œë°œ", "lat": first_place["lat"], "lng": first_place["lng"], "stay": 0, "type": "depot"})

    for p in places:
        nodes.append({"name": p["name"], "category": p["category"], "lat": p.get("lat"), "lng": p.get("lng"), "stay": stay_time_map.get(p["category"], 60), "type": "spot"})

    if restaurants:
        nodes.append({"name": restaurants[0]["name"], "category": "ì‹ë‹¹", "lat": restaurants[0].get("lat"), "lng": restaurants[0].get("lng"), "stay": 70, "type": "lunch"})
        nodes.append({"name": restaurants[1]["name"], "category": "ì‹ë‹¹", "lat": restaurants[1].get("lat"), "lng": restaurants[1].get("lng"), "stay": 70, "type": "dinner"})

    nodes.extend(build_fixed_nodes(fixed_events, day_start_dt))
    return nodes

def build_time_windows(nodes, day_start_dt):
    windows = []
    def get_rel(t_str): return int((parse_time(t_str) - day_start_dt).total_seconds() / 60)
    
    l_s, l_e = get_rel(LUNCH_WINDOW[0]), get_rel(LUNCH_WINDOW[1])
    d_s, d_e = get_rel(DINNER_WINDOW[0]), get_rel(DINNER_WINDOW[1])

    for n in nodes:
        if n["type"] == "lunch": windows.append((l_s, l_e))
        elif n["type"] == "dinner": windows.append((d_s, d_e))
        elif n["type"] == "fixed": windows.append(n["window"])
        else: windows.append((0, 24 * 60))
    return windows

# optimize_day í•¨ìˆ˜ëŠ” ê¸°ì¡´ ë¡œì§ì„ ìœ ì§€í•˜ë˜, get_all_detailed_pathsì—ì„œ ì¢Œí‘œ ì—†ëŠ” êµ¬ê°„ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ë¯€ë¡œ
# ì´í›„ ë¡œì§ì€ ëŒ€ë¶€ë¶„ ê·¸ëŒ€ë¡œ ë™ì‘í•œë‹¤.

def optimize_day(places, restaurants, fixed_events, start_time_str, target_date_str, end_time_str=None):
    TRAVEL_BUFFER = 5
    day_start_dt = datetime.strptime(start_time_str, "%H:%M")
    
    SAFE_GTFS_DATE = target_date_str
    r5_departure_dt = datetime.combine(datetime.strptime(SAFE_GTFS_DATE, "%Y-%m-%d"), datetime.strptime("11:00", "%H:%M").time())
    display_start_dt = datetime.combine(datetime.strptime(target_date_str, "%Y-%m-%d"), day_start_dt.time())

    max_horizon_minutes = 24 * 60
    if end_time_str:
        diff = int((datetime.strptime(end_time_str, "%H:%M") - day_start_dt).total_seconds() / 60)
        if diff > 0: max_horizon_minutes = diff

    nodes = build_nodes(places, restaurants, fixed_events, day_start_dt)
    for idx, node in enumerate(nodes): node["id"] = idx
    n = len(nodes)

    r5_travel_times = get_r5py_matrix(nodes, r5_departure_dt)
    time_matrix = [[0]*n for _ in range(n)]
    
    for i in range(n):
        for j in range(n):
            if i == j: continue
            val = r5_travel_times.get((i, j))
            if val is None: val = travel_minutes(nodes[i], nodes[j])
            
            # ê³ ì •ì¼ì • ì´ë™ì‹œê°„ ë³´ì •
            if (nodes[i]["type"]=="fixed" or nodes[j]["type"]=="fixed"):
                if not (nodes[i]["type"]=="depot" and nodes[j]["type"]=="fixed"):
                    val = max(val, 30)
            
            time_matrix[i][j] = nodes[i]["stay"] + int(val)

    # OR-Tools Solver
    manager = pywrapcp.RoutingIndexManager(n, 1, 0)
    routing = pywrapcp.RoutingModel(manager)

    def time_callback(from_idx, to_idx):
        return time_matrix[manager.IndexToNode(from_idx)][manager.IndexToNode(to_idx)]

    transit_callback = routing.RegisterTransitCallback(time_callback)
    routing.SetArcCostEvaluatorOfAllVehicles(transit_callback)
    routing.AddDimension(transit_callback, 30, max_horizon_minutes, False, "Time")
    time_dim = routing.GetDimensionOrDie("Time")

    time_windows = build_time_windows(nodes, day_start_dt)
    solver = routing.solver()

    for i, node in enumerate(nodes):
        index = manager.NodeToIndex(i)
        if node["type"] == "depot": continue
        
        window = time_windows[i]
        if node["type"] == "fixed":
            time_dim.CumulVar(index).SetRange(max(0, window[0]), min(max_horizon_minutes, window[1]))
            continue

        overlap_start, overlap_end = max(0, window[0]), min(max_horizon_minutes, window[1])
        if overlap_start > overlap_end:
            routing.AddDisjunction([index], 0)
            solver.Add(routing.VehicleVar(index) == -1)
        else:
            time_dim.CumulVar(index).SetRange(overlap_start, overlap_end)
            penalty = 1000000 if node["type"] in ["lunch", "dinner"] else 100000
            routing.AddDisjunction([index], penalty)

    search_params = pywrapcp.DefaultRoutingSearchParameters()
    # search_params.local_search_metaheuristic = routing_enums_pb2.LocalSearchMetaheuristic.GUIDED_LOCAL_SEARCH
    # search_params.time_limit.seconds = 1
    # search_params.log_search = False

    solution = routing.SolveWithParameters(search_params)
    if not solution: return []

    index = routing.Start(0)
    visited_nodes = []
    while not routing.IsEnd(index):
        node_idx = manager.IndexToNode(index)
        nodes[node_idx]['arrival_min'] = solution.Value(time_dim.CumulVar(index))
        visited_nodes.append(nodes[node_idx])
        index = solution.Value(routing.NextVar(index))

    trip_legs = [(visited_nodes[i], visited_nodes[i+1]) for i in range(len(visited_nodes)-1)]
    
    print("ğŸš€ ìƒì„¸ ê²½ë¡œ ê³„ì‚° ì¤‘...")
    start_path_time = time.time()
    path_map = get_all_detailed_paths(trip_legs, r5_departure_dt)
    end_path_time = time.time()
    print(f"â± ìƒì„¸ ê²½ë¡œ ê³„ì‚° ì™„ë£Œ: {round(end_path_time - start_path_time, 2)}ì´ˆ")

    def build_timeline_by_type(path_type):
        timeline = []
        actual_visits = [n for n in visited_nodes if n["type"] != "depot"]
        cursor = display_start_dt + timedelta(minutes=actual_visits[0]['arrival_min'])

        for i, node in enumerate(actual_visits):
            transit_info = []
            travel_min = 0
            
            if i > 0:
                prev = actual_visits[i-1]
                path_options = path_map.get((prev['id'], node['id']))
                
                if path_options:
                    chosen_path = path_options.get(path_type, path_options.get('fastest', []))
                    transit_info = chosen_path
                    for segment in chosen_path:
                        mins = re.findall(r'(\d+)ë¶„', segment)
                        for m in mins: travel_min += int(m)
                else:
                    # path_mapì— ì •ë³´ê°€ ì—†ìœ¼ë©´ ì¢Œí‘œ ìœ ë¬´ ê¸°ë°˜ í´ë°± ì ìš©
                    if prev.get('lat') is None or node.get('lat') is None:
                        travel_min = FALLBACK_MOVE_MIN
                        transit_info = [f"ë„ë³´ : {FALLBACK_MOVE_MIN}ë¶„"]
                    else:
                        dist = haversine(prev['lat'], prev['lng'], node['lat'], node['lng'])
                        travel_min = int(dist * 15)
                        transit_info = [f"ë„ë³´ : {travel_min}ë¶„"]

            if node["type"] == "fixed":
                time_parts = node.get("orig_time_str", "00:00 - 00:00").split(" - ")
                start_dt = datetime.strptime(f"{target_date_str} {time_parts[0]}", "%Y-%m-%d %H:%M")
                end_dt = datetime.strptime(f"{target_date_str} {time_parts[1]}", "%Y-%m-%d %H:%M")
                cursor = end_dt
                time_str = node["orig_time_str"]
            else:
                start_dt = cursor + timedelta(minutes=travel_min)
                end_dt = start_dt + timedelta(minutes=node["stay"])
                time_str = f"{start_dt.strftime('%H:%M')} - {end_dt.strftime('%H:%M')}"
                cursor = end_dt

            timeline.append({
                "name": node["name"],
                "category": node["category"],
                "time": time_str,
                "transit_to_here": transit_info
            })
        return timeline

    return {
        "fastest_version": build_timeline_by_type("fastest"),
        "min_transfer_version": build_timeline_by_type("min_transfer")
    }

# ============================================================
# 6. ë©”ì¸ ì‹¤í–‰ë¶€ (í†µí•©)
# ============================================================
if __name__ == "__main__":
    # # 1. ì—‘ì…€ ë° ê¸°ë³¸ ì •ë³´ ë¡œë“œ
    # print("ğŸ“‚ ì¥ì†Œ ë°ì´í„° ë¡œë“œ ì¤‘ (places_3000.xlsx)...")
    # try:
    #     df = pd.read_excel("places_3000.xlsx")
    # except FileNotFoundError:
    #     print("âŒ 'places_3000.xlsx' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    #     exit()

    # area = input("ì—¬í–‰í•  ì§€ì—­ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì¢…ë¡œêµ¬): ")
    
    # # 2. ì¥ì†Œ í•„í„°ë§
    # filtered_spot = df[(df["area"] == f"{area}") & (df["category"] != "ì‹ë‹¹")][["name", "lat", "lng"]]
    # filtered_restaurant = df[(df["area"] == f"{area}") & (df["category"] == "ì‹ë‹¹")][["name", "lat", "lng"]]
    # filtered_accom = df[(df["area"] == f"{area}") & (df["category"] == "ìˆ™ë°•")][["name", "lat", "lng"]]

    # places = filtered_spot.to_dict(orient="records")
    # restaurants = filtered_restaurant.to_dict(orient="records")
    # accommodations = filtered_accom.to_dict(orient="records")

    # 3. ë‚ ì§œ ì…ë ¥
    start_date = input("ì—¬í–‰ ì‹œì‘ ì¼ì (ì˜ˆ: 2026-01-20): ")
    end_date = input("ì—¬í–‰ ì¢…ë£Œ ì¼ì (ì˜ˆ: 2026-01-25): ")
    
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")
    days = (end - start).days + 1
    print(f"ì´ ì—¬í–‰ ì¼ìˆ˜: {days}ì¼")

    # # 4. Gemini API í˜¸ì¶œ (1ì°¨ ê³„íš ìƒì„±)
    # schema = """
    # {
    #   "plans": {
    #     "day1": {
    #       "route": [
    #         {"name": "...", "category": "...", "lat": 0.0, "lng": 0.0}
    #       ],
    #       "restaurants": [
    #         {"name": "...", "category": "ì‹ë‹¹", "lat": 0.0, "lng": 0.0}
    #       ],
    #       "accommodations": [
    #         {"name": "...", "category": "ìˆ™ë°•", "lat": 0.0, "lng": 0.0}
    #       ]
    #     }
    #   }
    # }
    # """
    
    # system_prompt = f"""
    # ë„ˆëŠ” ì„œìš¸ ì—¬í–‰ ì¥ì†Œ ì¶”ì²œê¸°ë‹¤. ë°˜ë“œì‹œ ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•œë‹¤.
    # {schema}
    # ê·œì¹™:
    # - ì…ë ¥ëœ days ë§Œí¼ day1, day2, ... ìƒì„±
    # - ì—¬í–‰ ì‹œì‘ ì¼ì : {start_date}, ì—¬í–‰ ì¢…ë£Œ ì¼ì : {end_date}
    # - ë§¤ì¼ ê´€ê´‘ì§€ 5ê³³ + ì‹ë‹¹ 2ê³³ êµ¬ì„±
    # - routeì—ëŠ” places ëª©ë¡ì—ì„œë§Œ ì„ íƒ
    # - restaurantsì—ëŠ” restaurants ëª©ë¡ì—ì„œë§Œ ì„ íƒ
    # - accommodationsì—ëŠ” accommodations ëª©ë¡ì—ì„œë§Œ ì„ íƒ
    # - routeëŠ” ì´ë™ ë™ì„ ì„ ê³ ë ¤í•˜ì—¬ ë°©ë¬¸ ìˆœì„œ ìµœì í™”
    # - restaurantsëŠ” í•´ë‹¹ dayì˜ ë§ˆì§€ë§‰ ê´€ê´‘ì§€ì™€ ê°€ê¹Œìš´ ìˆœì„œë¡œ 2ê³³ ì„ íƒ
    # - accommodationsëŠ” í•´ë‹¹ dayì˜ ë§ˆì§€ë§‰ ê´€ê´‘ì§€ì™€ ê°€ê¹Œìš´ ìˆœì„œë¡œ 1ê³³ ì„ íƒ
    # - ë§ˆì§€ë§‰ ë‚ ì—ëŠ” accommodations í¬í•¨í•˜ì§€ ì•ŠìŒ
    # - ì„¤ëª… ë¬¸ì¥ì€ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤
    # - ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•œë‹¤
    # """

    # user_prompt = {
    #     "days": days,
    #     "start_location": {"lat": 37.5547, "lng": 126.9706},
    #     "places": places[:6 * days * 3],
    #     "restaurants": restaurants[:3 * days * 3],
    #     "accommodations": accommodations[:days * 3]
    # }

    # print("ğŸ¤– Geminiê°€ ì´ˆê¸° ê³„íšì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    # prompt = system_prompt + "\n\n" + json.dumps(user_prompt, ensure_ascii=False)
    
    # start_time = time.time()
    # response = client.models.generate_content(model="gemini-2.5-flash-lite", contents=prompt)
    # print(f"â± Gemini ì‘ë‹µ ì‹œê°„: {round(time.time() - start_time, 3)}ì´ˆ")

    # try:
    #     result = extract_json(response.text)
    #     # result.json ì €ì¥ (ë°±ì—…ìš©)
    #     with open("result.json", "w", encoding="utf-8") as f:
    #         json.dump(result, f, ensure_ascii=False, indent=2)
    # except Exception as e:
    #     print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
    #     exit()

    result = json.load(open("result.json", "r", encoding="utf-8"))

    # 5. ì„¸ë¶€ ì¼ì • ì„¤ì •
    first_day_start_str = input("ì—¬í–‰ ì²«ë‚  ì‹œì‘ ì‹œê°„ (ì˜ˆ: 14:00) : ").strip() or "10:00"
    last_day_end_str = input("ì—¬í–‰ ë§ˆì§€ë§‰ ë‚  ì¢…ë£Œ ì‹œê°„ (ì˜ˆ: 18:00) : ").strip() or "21:00"
    default_start_str = "10:00"
    default_end_str = "21:00"

    FIXED_EVENTS = []
    if input("ê³ ì • ì¼ì •ì´ ìˆë‚˜ìš”? (y/n): ").strip().lower() == "y":
        while True:
            FIXED_EVENTS.append({
                "date": input("ë‚ ì§œ (YYYY-MM-DD): "),
                "title": input("ì œëª©: "),
                "start": input("ì‹œì‘(HH:MM): "),
                "end": input("ì¢…ë£Œ(HH:MM): ")
            })
            if input("ë” ì¶”ê°€í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() != "y": break

    # 6. ìµœì í™” ì‹¤í–‰ (Day loop)
    plans = result["plans"]
    day_keys = list(plans.keys())

    print(f"\nğŸš€ ë³‘ë ¬ ìµœì í™” ì‹œì‘: {len(day_keys)}ì¼ì¹˜ ì¼ì •ì„ ë™ì‹œì— ê³„ì‚°í•©ë‹ˆë‹¤.")
    start_total_opt = time.time()

    # [ë‚´ë¶€ í•¨ìˆ˜] ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë˜í¼ í•¨ìˆ˜
    def process_day_wrapper(args):
        day_key, date_obj, is_first, is_last = args
        
        todays_start = first_day_start_str if is_first else default_start_str
        todays_end = last_day_end_str if is_last else default_end_str
        current_date_str = date_obj.strftime("%Y-%m-%d")
        
        print(f"   â–¶ {day_key} ìµœì í™” ì‹œì‘...")
        
        # ì‹¤ì œ ìµœì í™” ìˆ˜í–‰
        day_res = optimize_day(
            places=plans[day_key]["route"],
            restaurants=plans[day_key]["restaurants"],
            fixed_events=get_fixed_events_for_day(FIXED_EVENTS, current_date_str),
            start_time_str=todays_start,
            target_date_str=current_date_str,
            end_time_str=todays_end
        )
        return day_key, day_res

    # 6-1. ë³‘ë ¬ ì‹¤í–‰ ì¸ì(Task) ì¤€ë¹„
    tasks = []
    curr = start
    for i, day_key in enumerate(day_keys):
        tasks.append((day_key, curr, i==0, i==len(day_keys)-1))
        curr += timedelta(days=1)

    # 6-2. ThreadPoolExecutorë¡œ ë³‘ë ¬ ì‹¤í–‰
    processed_results = {}

    with ThreadPoolExecutor(max_workers=JAVA_PARALLELISM) as executor:
        for day_key, day_res in executor.map(process_day_wrapper, tasks):
            processed_results[day_key] = day_res
            print(f"   âœ… {day_key} ì™„ë£Œ")

    print(f"â± ì „ì²´ ìµœì í™” ì™„ë£Œ: {round(time.time() - start_total_opt, 2)}ì´ˆ")
    
    # 3. ê²°ê³¼ ì·¨í•© ë° í™”ë©´ ì¶œë ¥
    curr = start
    for i, day_key in enumerate(day_keys):
        # ê²°ê³¼ ì €ì¥
        result["plans"][day_key]["timelines"] = processed_results[day_key]
        day_results = processed_results[day_key]
        
        print(f"\nğŸ“… {day_key} ({curr.strftime('%Y-%m-%d')})")

        # ë‘ ê°€ì§€ ë²„ì „(ìµœë‹¨ ì‹œê°„, ìµœì†Œ í™˜ìŠ¹) ëª¨ë‘ ì¶œë ¥
        for ver_key, label in [("fastest_version", "ìµœë‹¨ ì‹œê°„"), ("min_transfer_version", "ìµœì†Œ í™˜ìŠ¹")]:
            timeline = day_results[ver_key]
            
            separator = "-" * 60
            print(f"\n[{label} ê¸°ì¤€ ì¼ì •] {day_key}")
            print(separator)

            for t in timeline:
                if t.get('transit_to_here'):
                    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ê²½ë¡œë¥¼ í™”ì‚´í‘œë¡œ ì—°ê²°í•˜ì—¬ ì¶œë ¥
                    path_str = " -> ".join([s for s in t['transit_to_here']])
                    print(f"  [TRANSIT] {path_str}")
                print(f"  [{t['time']}] {t['name']} ({t['category']})")
            
            print(separator)

        # ë‚ ì§œ ì¹´ìš´í„° ì¦ê°€
        curr += timedelta(days=1)

    # 7. ëª¨ë“  ë£¨í”„ê°€ ëë‚œ í›„ ìµœì¢… íŒŒì¼ ì €ì¥ (ë£¨í”„ ì™¸ë¶€)
    with open("result_timeline.json", "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
        print("\nì „ì²´ ì¼ì •ì´ 'result_timeline.json' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")