import os
import multiprocessing
import joblib  # ëª¨ë¸ ë¡œë“œë¥¼ ìœ„í•´ ì¶”ê°€
import numpy as np # ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€

available_cores = multiprocessing.cpu_count()
JAVA_PARALLELISM = 1
print(f"âš™ï¸  ì„¤ì •ëœ ì‚¬ìš© ì½”ì–´ ìˆ˜: {JAVA_PARALLELISM}ê°œ")
# JAVA_HOME ê²½ë¡œëŠ” ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ í™•ì¸ í•„ìš”
os.environ["JAVA_HOME"] = r"C:\Program Files\Java\jdk-21.0.10"
os.environ["JAVA_OPTS"] = f"-Xmx8G -Djava.util.concurrent.ForkJoinPool.common.parallelism={JAVA_PARALLELISM}"

from google import genai
import zipfile
import json
import pandas as pd
import geopandas as gpd
import math
from datetime import datetime, timedelta
from dotenv import load_dotenv
from ortools.constraint_solver import routing_enums_pb2, pywrapcp
import time
import re
from r5py import TransportNetwork, TravelTimeMatrix, DetailedItineraries, TransportMode
import pickle
from concurrent.futures import ThreadPoolExecutor

# ============================================================
# 1. í™˜ê²½ ì„¤ì • ë° ì „ì—­ ìƒìˆ˜
# ============================================================ 
load_dotenv()
API_KEY = os.getenv("API_KEY")
client = genai.Client(api_key=API_KEY)

# ìºì‹œ ì €ì¥ì†Œ
DETAILED_PATH_CACHE = {}

# í´ë°±(ì¢Œí‘œ ì—†ëŠ” ê²½ìš°) ì´ë™ ì‹œê°„ ì„¤ì •(ë¶„)
FALLBACK_MOVE_MIN = 30

# ë„ë³´ ì´ë™ ì œí•œ
WALK_ONLY_THRESHOLD_MIN = 12   
WALK_ONLY_THRESHOLD_MAX = 18   

MAX_TRANSFERS = 2
MAX_TRAVEL_TIME_MIN = 90

# ì‹œê°„ ìœˆë„ìš° ì„¤ì •
LUNCH_WINDOW = ("11:20", "13:20")
DINNER_WINDOW = ("17:40", "19:30")

stay_time_map = {
    "ê´€ê´‘ì§€": 90, "ì¹´í˜": 50, "ìŒì‹ì ": 70, 
    "ë°•ë¬¼ê´€": 120, "ê³µì›": 60, "ì‹œì¥": 80, "ìˆ™ë°•": 0
}

osm_file = "./data/seoul_osm_v.pbf"
gtfs_files = ["./data/seoul_area_gtfs.zip"]

# [ì¶”ê°€] í˜¼ì¡ë„ ëª¨ë¸ ê´€ë ¨ ìƒìˆ˜ ë° ê°€ì¤‘ì¹˜
MODEL_PATH = "./model/seoul_congestion_model.pkl"
TRAFFIC_WEIGHTS = {0: 1.0, 1: 1.3, 2: 2}  # Low, Medium, High
CROWD_WEIGHTS = {0: 1.0, 1: 1.1, 2: 1.3}    # Low, Medium, High 
# [ìˆ˜ì •] ì´ëª¨ì§€ ë§¤í•‘ (0:Low, 1:Medium, 2:High)
EMOJI_MAP = {0: "ğŸŸ¢", 1: "ğŸŸ¡", 2: "ğŸ”´"}


# [NEW] ì œê³µí•´ì£¼ì‹  êµí†µ ì§€ì  ì¢Œí‘œ ë°ì´í„° (ìœ íš¨í•˜ì§€ ì•Šì€ 0.0 ì¢Œí‘œ ì œì™¸)
TRAFFIC_NODE_COORDS = {
    "ì„±ì‚°ë¡œ(ê¸ˆí™”í„°ë„)": (37.56859, 126.94844),
    "ì‚¬ì§ë¡œ(ì‚¬ì§í„°ë„)": (37.57231, 126.96325),
    "ìí•˜ë¬¸ë¡œ(ìí•˜ë¬¸í„°ë„)": (37.58883, 126.96855),
    "ëŒ€ì‚¬ê´€ë¡œ(ì‚¼ì²­í„°ë„)": (37.59636, 126.98421),
    "ìœ¨ê³¡ë¡œ(ì•ˆêµ­ì—­)": (37.57600, 126.98434),
    "ì°½ê²½ê¶ë¡œ(ì„œìš¸ì—¬ìëŒ€í•™êµ)": (37.58253, 126.99801),
    "ëŒ€í•™ë¡œ(í•œêµ­ë°©ì†¡í†µì‹ ëŒ€í•™êµ)": (37.57820, 127.00202),
    "ì¢…ë¡œ(ë™ë¬˜ì•ì—­)": (37.57345, 127.01676),
    "í‡´ê³„ë¡œ(ì‹ ë‹¹ì—­)": (37.56571, 127.02091),
    "ë™í˜¸ë¡œ(ì¥ì¶©ì²´ìœ¡ê´€)": (37.55879, 127.00725),
    "ì¥ì¶©ë‹¨ë¡œ(ì¥ì¶©ë‹¨ê³µì›)": (37.55689, 127.00466),
    "í‡´ê³„ë¡œ(íšŒí˜„ì—­)": (37.55743, 126.97649),
    "ì„¸ì¢…ëŒ€ë¡œ(ì„œìš¸ì—­)": (37.55912, 126.97454),
    "ìƒˆë¬¸ì•ˆë¡œ(ì„œìš¸ì—­ì‚¬ë°•ë¬¼ê´€)": (37.56965, 126.97144),
    "ì¢…ë¡œ(ì¢…ë¡œ3ê°€ì—­)": (37.57053, 126.99096),
    "ì„œì†Œë¬¸ë¡œ(ì‹œì²­ì—­)": (37.56272, 126.97307),
    "ì„¸ì¢…ëŒ€ë¡œ(ì‹œì²­ì—­2)": (37.56750, 126.97721),
    "ì„ì§€ë¡œ(ì„ì§€ë¡œ3ê°€ì—­)": (37.56632, 126.98910),
    "ì¹ íŒ¨ë¡œ(ìˆ­ë¡€ë¬¸)": (37.55959, 126.97252),
    "ë‚¨ì‚°1í˜¸í„°ë„": (37.54241, 127.00136),
    "ë‚¨ì‚°2í˜¸í„°ë„": (37.55532, 126.98977),
    "ë‚¨ì‚°3í˜¸í„°ë„": (37.54478, 126.98835),
    "ì†Œì›”ë¡œ(íšŒí˜„ì—­)": (37.55704, 126.97636),
    "ì†ŒíŒŒë¡œ(ìˆ­ì˜ì—¬ìëŒ€í•™êµ)": (37.55490, 126.98352),
    "ë„ë´‰ë¡œ(ë„ë´‰ì‚°ì—­)": (37.69179, 127.04509),
    "ë™ì¼ë¡œ(ì˜ì •ë¶€IC)": (37.68857, 127.05538),
    "ì•„ì°¨ì‚°ë¡œ(ì›Œì»¤í)": (37.55010, 127.10841),
    "ë§ìš°ë¡œ(ë§ìš°ë¦¬ê³µì›)": (37.60148, 127.11567),
    "ê²½ì¶˜ë¶ë¡œ(ì¤‘ë‘ê²½ì°°ì„œ)": (37.61994, 127.10533),
    "í™”ë‘ë¡œ(ì¡°ì„ ì™•ë¦‰)": (37.63088, 127.09911),
    "ë¶ë¶€ê°„ì„ ë„ë¡œ(ì‹ ë‚´IC)": (37.61400, 127.10836),
    "ì„œí•˜ë‚¨ë¡œ(ì„œí•˜ë‚¨IC)": (37.51684, 127.14690),
    "ì²œí˜¸ëŒ€ë¡œ(ìƒì¼IC)": (37.54746, 127.17522),
    "ì˜¬ë¦¼í”½ëŒ€ë¡œ(ê°•ì¼IC)": (37.56708, 127.14080),
    "ê²½ë¶€ê³ ì†ë„ë¡œ(ì–‘ì¬IC)": (37.46516, 127.03868),
    "ì†¡íŒŒëŒ€ë¡œ(ë³µì •ì—­)": (37.46816, 127.12651),
    "ë°¤ê³ ê°œë¡œ(ì„¸ê³¡ë™ì‚¬ê±°ë¦¬)": (37.46242, 127.10788),
    "ë¶„ë‹¹ìˆ˜ì„œë¡œ(ì„±ë‚¨ì‹œê³„)": (37.47103, 127.12321),
    "ê³¼ì²œëŒ€ë¡œ(ë‚¨íƒœë ¹ì—­)": (37.46329, 126.98815),
    "ì–‘ì¬ëŒ€ë¡œ(ì–‘ì¬IC)": (37.46009, 127.03018),
    "ë°˜í¬ëŒ€ë¡œ(ìš°ë©´ì‚°í„°ë„)": (37.48372, 127.01184),
    "ì‹œí¥ëŒ€ë¡œ(ì„ìˆ˜ì—­)": (37.43701, 126.90281),
    "ê¸ˆì˜¤ë¡œ(ê´‘ëª…ì‹œê³„)": (37.48243, 126.84189),
    "ì˜¤ë¦¬ë¡œ(ê´‘ëª…ì‹œê³„)": (37.48261, 126.84311),
    "ê°œë´‰ë¡œ(ê°œë´‰êµ)": (37.48618, 126.85650),
    "ê´‘ëª…ëŒ€êµ(ê´‘ëª…ì‹œê³„)": (37.48504, 126.87330),
    "ì² ì‚°êµ(ê´‘ëª…ì‹œê³„)": (37.47510, 126.87835),
    "ê¸ˆì²œêµ(ê´‘ëª…ì‹œê³„)": (37.46517, 126.88425),
    "ê¸ˆí•˜ë¡œ(ê´‘ëª…ì‹œê³„)": (37.45135, 126.89169),
    "ì˜¤ì •ë¡œ(ë¶€ì²œì‹œê³„)": (37.54278, 126.80941),
    "í™”ê³¡ë¡œ(í™”ê³¡ë¡œì…êµ¬)": (37.53935, 126.82308),
    "ê²½ì¸ê³ ì†êµ­ë„(ì‹ ì›”IC)": (37.52485, 126.83186),
    "ê²½ì¸ë¡œ(ìœ í•œê³µê³ )": (37.48861, 126.82279),
    "ì‹ ì •ë¡œ(ì‘ë™í„°ë„)": (37.50607, 126.82458),
    "ê¹€í¬ëŒ€ë¡œ(ê°œí™”êµ)": (37.58531, 126.79557),
    "ì˜¬ë¦¼í”½ëŒ€ë¡œ(ê°œí™”IC)": (37.58776, 126.81297),
    "í†µì¼ë¡œ(ê³ ì–‘ì‹œê³„)": (37.64469, 126.91127),
    "ì„œì˜¤ë¦‰ë¡œ(ê³ ì–‘ì‹œê³„)": (37.61788, 126.90626),
    "ìˆ˜ìƒ‰ë¡œ(ê³ ì–‘ì‹œê³„)": (37.58747, 126.88641),
    "ê°•ë³€ë¶ë¡œ(ë‚œì§€í•œê°•ê³µì›)": (37.57089, 126.87256),
    "ê°•ë³€ë¶ë¡œ(êµ¬ë¦¬ì‹œê³„)": (37.55827, 127.11420),
    "ë™ë¶€ê°„ì„ ë„ë¡œ(ìƒë„ì§€í•˜ì°¨ë„)": (37.68325, 127.05253),
    "í–‰ì£¼ëŒ€êµ": (37.59812, 126.80993),
    "ì›”ë“œì»µëŒ€êµ": (37.55647, 126.88551),
    "ê°€ì–‘ëŒ€êµ": (37.57157, 126.86273),
    "ì„±ì‚°ëŒ€êµ": (37.54820, 126.88895),
    "ì–‘í™”ëŒ€êµ": (37.54279, 126.90349),
    "ì„œê°•ëŒ€êµ": (37.53736, 126.92526),
    "ë§ˆí¬ëŒ€êµ": (37.53360, 126.93658),
    "ì›íš¨ëŒ€êµ": (37.52424, 126.94046),
    "í•œê°•ëŒ€êµ": (37.51811, 126.95929),
    "ë™ì‘ëŒ€êµ": (37.50976, 126.98181),
    "ë°˜í¬ëŒ€êµ": (37.51462, 126.99667),
    "ì ìˆ˜êµ": (37.50826, 126.99974),
    "í•œë‚¨ëŒ€êµ": (37.52711, 127.01328),
    "ë™í˜¸ëŒ€êµ": (37.53814, 127.02001),
    "ì„±ìˆ˜ëŒ€êµ": (37.53685, 127.03511),
    "ì˜ë™ëŒ€êµ": (37.53041, 127.05746),
    "ì²­ë‹´ëŒ€êµ": (37.52840, 127.06544),
    "ì ì‹¤ëŒ€êµ": (37.52409, 127.09204),
    "ì˜¬ë¦¼í”½ëŒ€êµ": (37.53387, 127.10423),
    "ì²œí˜¸ëŒ€êµ": (37.54267, 127.11288),
    "ê´‘ì§„êµ": (37.54415, 127.11526),
    "ì§„í¥ë¡œ(êµ¬ê¸°í„°ë„)": (37.60869, 126.95531),
    "í‰ì°½ë¬¸í™”ë¡œ(ë¶ì•…í„°ë„)": (37.61155, 126.97931),
    "ë™í˜¸ë¡œ(ê¸ˆí˜¸í„°ë„)": (37.55178, 127.01320),
    "ì„œë¹™ê³ ë¡œ(í•œë‚¨ì—­)": (37.52720, 127.00470),
    "ì²œí˜¸ëŒ€ë¡œ(êµ°ìêµ)": (37.56072, 127.06857),
    "ëšì„¬ë¡œ(ìš©ë¹„êµ)": (37.54201, 127.02067),
    "ë™ì¼ë¡œ(êµ°ìêµ)": (37.55381, 127.07050),
    "í™”ë‘ë¡œ(ìƒì›”ê³¡ì—­)": (37.60422, 127.04427),
    "ë™ì†Œë¬¸ë¡œ(ê¸¸ìŒêµì‚¬ê±°ë¦¬)": (37.59921, 127.02177),
    "í™”ë‘ë¡œ(í™”ë‘ëŒ€ì—­)": (37.61950, 127.08093),
    "ë„ë´‰ë¡œ(ìŒë¬¸ì—­)": (37.64571, 127.03317),
    "ë™ë¶€ê°„ì„ ë„ë¡œ(ì›”ê³„1êµ)": (37.63148, 127.06358),
    "ë™ì¼ë¡œ(ë…¸ì›ì—­)": (37.65247, 127.06093),
    "ì¦ì‚°ë¡œ(ë””ì§€í„¸ë¯¸ë””ì–´ì‹œí‹°ì—­)": (37.57968, 126.90512),
    "í†µì¼ë¡œ(ì‚°ê³¨ê³ ê°œì •ë¥˜ì¥)": (37.59493, 126.94025),
    "ì„±ì‚°ë¡œ(ì—°í¬IC)": (37.56351, 126.93010),
    "ì—°í¬ë¡œ(ì—°í¬IC)": (37.56626, 126.93054),
    "ë‚¨ë¶€ìˆœí™˜ë¡œ(í™”ê³¡ë¡œì…êµ¬ êµì°¨ë¡œ)": (37.53997, 126.82539),
    "ë‚¨ë¶€ìˆœí™˜ë¡œ(ì‹ ì›”IC)": (37.52277, 126.83643),
    "ê°•ì„œë¡œ(í™”ê³¡í„°ë„)": (37.53446, 126.84511),
    "ê³µí•­ëŒ€ë¡œ(ë°œì‚°ì—­)": (37.55902, 126.83178),
    "ê²½ì¸ë¡œ(ì˜¤ë¥˜IC)": (37.49798, 126.85170),
    "ê²½ì¸ë¡œ(ê±°ë¦¬ê³µì›ì…êµ¬êµì°¨ë¡œ)": (37.50644, 126.88438),
    "ì‹œí¥ëŒ€ë¡œ(ì‹œí¥IC)": (37.47753, 126.89904),
    "ì˜ë“±í¬ë¡œ(ì˜¤ëª©êµ)": (37.52318, 126.88373),
    "ì‹œí¥ëŒ€ë¡œ(êµ¬ë¡œë””ì§€í„¸ë‹¨ì§€ì—­)": (37.48741, 126.90545),
    "êµ­íšŒëŒ€ë¡œ(ì—¬ì˜2êµ)": (37.52665, 126.91333),
    "ê²½ì¸ë¡œ(ì„œìš¸êµ)": (37.52019, 126.91490),
    "ì—¬ì˜ëŒ€ë°©ë¡œ(ì—¬ì˜êµ)": (37.51684, 126.92838),
    "ì–‘ë…•ë¡œ(ìƒë„í„°ë„)": (37.51120, 126.95347),
    "ë™ì‘ëŒ€ë¡œ(ì´ì‹ ëŒ€ì…êµ¬ì—­)": (37.49435, 126.98291),
    "ë¬¸ì„±ë¡œ(ë‚œê³¡í„°ë„)": (37.47905, 126.92425),
    "ë‚¨ë¶€ìˆœí™˜ë¡œ(ë‚™ì„±ëŒ€ì—­)": (37.47771, 126.96224),
    "ë‚¨ë¶€ìˆœí™˜ë¡œ(ì˜ˆìˆ ì˜ì „ë‹¹)": (37.47622, 127.00482),
    "ê°•ë‚¨ëŒ€ë¡œ(ê°•ë‚¨ì—­-ì‹ ë¶„ë‹¹)": (37.49069, 127.03116),
    "ì‚¬í‰ëŒ€ë¡œ(ê³ ì†í„°ë¯¸ë„ì—­)": (37.50323, 127.00596),
    "ë°˜í¬ëŒ€ë¡œ(ì„œì´ˆì—­)": (37.49624, 127.00555),
    "ì–¸ì£¼ë¡œ(ë§¤ë´‰í„°ë„)": (37.49201, 127.04797),
    "ë‚¨ë¶€ìˆœí™˜ë¡œ(ìˆ˜ì„œIC)": (37.49610, 127.09103),
    "í—Œë¦‰ë¡œ(ì„¸ê³¡ë™ì‚¬ê±°ë¦¬)": (37.46516, 127.10576),
    "ë…¸ë“¤ë¡œ(ì—¬ì˜í•˜ë¥˜IC)": (37.52965, 126.90915),
    "í…Œí—¤ë€ë¡œ(ì„ ë¦‰ì—­)": (37.50548, 127.05213),
    "ê°•ë‚¨ëŒ€ë¡œ(ì‹ ì‚¬ì—­)": (37.51480, 127.02013),
    "ë°±ì œê³ ë¶„ë¡œ(ì¢…í•©ìš´ë™ì¥)": (37.51064, 127.07856),
    "ì†¡íŒŒëŒ€ë¡œ(ì†¡íŒŒì—­)": (37.50003, 127.11218),
    "ì„œë¶€ê°„ì„ ë„ë¡œ(ì§€ìƒ)": (37.52097, 126.88183),
    "ì˜¬ë¦¼í”½ëŒ€ë¡œ": (37.50600, 126.97375),
    "ê°•ë³€ë¶ë¡œ": (37.51700, 126.97412),
    "ë‚´ë¶€ìˆœí™˜ë¡œ": (37.60868, 126.99888),
    "ë¶ë¶€ê°„ì„ ë¡œ": (37.60856, 127.05258),
    "ë™ë¶€ê°„ì„ ë„ë¡œ": (37.56869, 127.07602),
    "ê²½ë¶€ê³ ì†ë„ë¡œ": (37.49321, 127.02252),
    "ë¶„ë‹¹ìˆ˜ì„œë¡œ": (37.49770, 127.08720),
    "ê°•ë‚¨ìˆœí™˜ë¡œ(ê´€ì•…í„°ë„)": (37.44910, 126.92617),
    "ì„œë¶€ê°„ì„ ì§€í•˜ë„ë¡œ": (37.46894, 126.88367),
    "ì‹ ì›”ì—¬ì˜ì§€í•˜ë„ë¡œ": (37.52932, 126.86228)
}

KOREAN_HOLIDAYS_2025 = {
    '20250101': 'ì‹ ì •', '20250128': 'ì„¤ë‚ ì—°íœ´', '20250129': 'ì„¤ë‚ ', '20250130': 'ì„¤ë‚ ì—°íœ´',
    '20250301': 'ì‚¼ì¼ì ˆ', '20250303': 'ëŒ€ì²´ê³µíœ´ì¼', '20250505': 'ì–´ë¦°ì´ë‚ ',
    '20250506': 'ëŒ€ì²´ê³µíœ´ì¼', '20250606': 'í˜„ì¶©ì¼', '20250815': 'ê´‘ë³µì ˆ',
    '20251003': 'ê°œì²œì ˆ', '20251005': 'ì¶”ì„ì—°íœ´', '20251006': 'ì¶”ì„',
    '20251007': 'ì¶”ì„ì—°íœ´', '20251008': 'ëŒ€ì²´ê³µíœ´ì¼', '20251009': 'í•œê¸€ë‚ ',
    '20251225': 'í¬ë¦¬ìŠ¤ë§ˆìŠ¤'
}

SEOUL_WEATHER_2025 = {
    1: {'temp': -2, 'rain_prob': 15}, 2: {'temp': 1, 'rain_prob': 15},
    3: {'temp': 6, 'rain_prob': 25}, 4: {'temp': 13, 'rain_prob': 30},
    5: {'temp': 18, 'rain_prob': 35}, 6: {'temp': 23, 'rain_prob': 50},
    7: {'temp': 26, 'rain_prob': 60}, 8: {'temp': 27, 'rain_prob': 45},
    9: {'temp': 22, 'rain_prob': 35}, 10: {'temp': 15, 'rain_prob': 25},
    11: {'temp': 7, 'rain_prob': 20}, 12: {'temp': 0, 'rain_prob': 20}
}

ROAD_CAPACITY_DEFAULT = {
    'í„°ë„': 2500, 'ëŒ€ë¡œ': 2000, 'ë¡œ': 1500, 'ì—­': 1800, 'default': 1600
}

# [ì¶”ê°€] ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ë¡œë“œ
try:
    if os.path.exists(MODEL_PATH):
        print(f"ğŸ“¦ í˜¼ì¡ë„ ëª¨ë¸ ë¡œë“œ ì¤‘: {MODEL_PATH}")
        loaded_package = joblib.load(MODEL_PATH)
        TRAFFIC_MODEL = loaded_package['traffic_model']
        CROWD_MODEL = loaded_package['crowd_model']
        LOCATION_MAP = loaded_package['location_map']
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    else:
        print("âš ï¸ ê²½ê³ : í˜¼ì¡ë„ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê°€ì¤‘ì¹˜ê°€ ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        TRAFFIC_MODEL, CROWD_MODEL, LOCATION_MAP = None, None, {}
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    TRAFFIC_MODEL, CROWD_MODEL, LOCATION_MAP = None, None, {}

# ============================================================
# 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================
def parse_time(t):
    return datetime.strptime(t, "%H:%M")

def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # km
    phi1, phi2 = math.radians(lat1), math.radians(lat2)
    dphi = math.radians(lat2 - lat1)
    dlambda = math.radians(lon2 - lon1)
    a = math.sin(dphi / 2) ** 2 + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2) ** 2
    return 2 * R * math.atan2(math.sqrt(a), math.sqrt(1 - a))

def approx_walk_minutes(start, end):
    if not start or not end or start.get("lat") is None or end.get("lat") is None:
        return FALLBACK_MOVE_MIN
    dist_km = haversine(start["lat"], start["lng"], end["lat"], end["lng"])
    return dist_km * 15

def dynamic_walk_threshold(dist_km):
    if dist_km < 0.6: return WALK_ONLY_THRESHOLD_MAX
    elif dist_km < 1.2: return 15
    else: return WALK_ONLY_THRESHOLD_MIN

def travel_minutes(p1, p2):
    if p1 is None or p2 is None or p1.get("lat") is None or p2.get("lat") is None: return 0
    dist = haversine(p1["lat"], p1["lng"], p2["lat"], p2["lng"])
    return int(dist / 30 * 60)

def get_fixed_events_for_day(fixed_events, target_date):
    return [e for e in fixed_events if e["date"] == target_date]

def duration_to_minutes(val):
    if val is None or pd.isna(val): return 0
    if hasattr(val, "total_seconds"): return int(val.total_seconds() / 60)
    if isinstance(val, str) and "day" in val:
        try: return int(pd.to_timedelta(val).total_seconds() / 60)
        except: return 0
    try: return int(float(val))
    except: return 0

def extract_json(text):
    if not text: raise ValueError("Gemini ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    text = text.strip()
    if text.startswith("```"):
        text = text.split("```")[1]
        if text.startswith("json"): text = text[4:]
    start, end = text.find("{"), text.rfind("}") + 1
    if start == -1 or end == -1: raise ValueError("JSON íŒŒì‹± ì‹¤íŒ¨:\n" + text)
    return json.loads(text[start:end])

# [ì¶”ê°€] í˜¼ì¡ë„ ì˜ˆì¸¡ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ë“¤
def get_road_capacity_val(name):
    name = str(name)
    if 'í„°ë„' in name: return 2500
    if 'ëŒ€ë¡œ' in name: return 2000
    if 'ì—­' in name: return 1800
    if 'ë¡œ' in name: return 1500
    return 1600

# [NEW] ê°€ì¥ ê°€ê¹Œìš´ êµí†µ ì§€ì  ì°¾ê¸°
def find_nearest_traffic_node(target_lat, target_lng, max_dist_km=2.0):
    if target_lat is None or target_lng is None:
        return None
        
    nearest_node = None
    min_dist = float('inf')

    for name, (node_lat, node_lng) in TRAFFIC_NODE_COORDS.items():
        # ëª¨ë¸ì— ì¡´ì¬í•˜ëŠ” ì§€ì ì¸ì§€ í™•ì¸
        if TRAFFIC_MODEL is not None and LOCATION_MAP and name not in LOCATION_MAP:
            continue
            
        dist = haversine(target_lat, target_lng, node_lat, node_lng)
        
        if dist < min_dist:
            min_dist = dist
            nearest_node = name

    if min_dist <= max_dist_km:
        return nearest_node
    else:
        return None

# [MODIFIED] ì˜ˆì¸¡ í•¨ìˆ˜ (ì¢Œí‘œ ê¸°ë°˜ ë§¤í•‘ ì¶”ê°€)
def predict_congestion_weights(location_name, current_dt, lat=None, lng=None):
    if TRAFFIC_MODEL is None or not LOCATION_MAP:
        return 1.0, 1.0, "âšª", "âšª"

    target_name = None

    # 1. ì´ë¦„ ì¼ì¹˜ í™•ì¸
    if location_name in LOCATION_MAP:
        target_name = location_name
    
    # 2. ì¢Œí‘œ ê¸°ë°˜ ë§¤í•‘ (ì´ë¦„ ë¶ˆì¼ì¹˜ ì‹œ)
    if target_name is None and lat is not None and lng is not None:
        found_node = find_nearest_traffic_node(lat, lng)
        if found_node:
            target_name = found_node

    if target_name is None:
        return 1.0, 1.0, "âšª", "âšª"

    hour = current_dt.hour
    month = current_dt.month
    day_of_week = current_dt.weekday() 
    date_str = current_dt.strftime("%Y%m%d")
    
    is_holiday = 1 if date_str in KOREAN_HOLIDAYS_2025 else 0
    is_weekend = 1 if day_of_week >= 5 else 0
    
    weather = SEOUL_WEATHER_2025.get(month, SEOUL_WEATHER_2025[1])
    rain_prob = weather['rain_prob']
    temp = weather['temp']
    
    rng = np.random.RandomState(month * 100 + hour)
    is_raining = 1 if rng.rand() < (rain_prob / 100) else 0
    weather_impact = 1.3 if is_raining else 1.0
    if is_raining: rain_prob = 80

    road_cap = get_road_capacity_val(target_name)
    loc_code = LOCATION_MAP[target_name]

    input_data = pd.DataFrame([{
        'location_code': loc_code,
        'hour': hour,
        'day_of_week': day_of_week,
        'month': month,
        'is_weekend': is_weekend,
        'is_holiday': is_holiday,
        'temperature': temp,
        'rain_prob': rain_prob,
        'weather_impact': weather_impact,
        'road_capacity': road_cap
    }])

    try:
        t_level = TRAFFIC_MODEL.predict(input_data)[0]
        c_level = CROWD_MODEL.predict(input_data)[0]
        
        t_weight = TRAFFIC_WEIGHTS.get(t_level, 1.0)
        c_weight = CROWD_WEIGHTS.get(c_level, 1.0)
        
        # [ìˆ˜ì •] ì´ëª¨ì§€ë¡œ ë°˜í™˜
        t_emoji = EMOJI_MAP.get(t_level, "âšª")
        c_emoji = EMOJI_MAP.get(c_level, "âšª")
        
        return t_weight, c_weight, t_emoji, c_emoji
    except Exception as e:
        return 1.0, 1.0, "âšª", "âšª"

# ============================================================
# 3. êµí†µ ë°ì´í„° ë¡œë“œ (GTFS & OSM)
# ============================================================

# 3-1. TransportNetwork (ê¸°ì¡´ ìœ ì§€)
pickle_path = "./data/seoul_tn_cached.pkl"
if os.path.exists(pickle_path):
    print(f"ğŸ“¦ TransportNetwork ë¡œë“œ ì¤‘...")
    transport_network = TransportNetwork.__new__(TransportNetwork)
    transport_network._transport_network = TransportNetwork._load_pickled_transport_network(self=TransportNetwork, path=pickle_path)
else:
    print("ğŸš€ TransportNetwork ìƒì„± ì¤‘...")
    transport_network = TransportNetwork(osm_file, gtfs_files)
    transport_network._save_pickled_transport_network(path=pickle_path, transport_network=transport_network)

# 3-2 & 3-3. ë©”íƒ€ë°ì´í„°(Stop/Route) ê³ ì† ë¡œë“œ (Pickle ì ìš©)
meta_cache_path = "./data/metadata_cache.pkl"

if os.path.exists(meta_cache_path):
    print("âš¡ ë©”íƒ€ë°ì´í„° ìºì‹œ ë¡œë“œ ì¤‘...")
    with open(meta_cache_path, "rb") as f:
        meta_data = pickle.load(f)
        STOP_ID_TO_NAME = meta_data["stops"]
        ROUTE_ID_TO_NAME = meta_data["routes"]
        STOP_ROUTE_MAP = meta_data["stop_route_map"]
else:
    print("ğŸ¢ ë©”íƒ€ë°ì´í„° ìƒì„± ì¤‘ (ìµœì´ˆ 1íšŒë§Œ ëŠë¦¼)...")
    # Stops
    with zipfile.ZipFile(gtfs_files[0]) as z:
        with z.open("stops.txt") as f:
            stops_df = pd.read_csv(f, dtype={'stop_id': str})
    STOP_ID_TO_NAME = {str(row['stop_id']).strip(): str(row['stop_name']).strip() for _, row in stops_df.iterrows()}
    
    # Routes
    with zipfile.ZipFile(gtfs_files[0]) as z:
        with z.open("routes.txt") as f:
            routes_df = pd.read_csv(f)
    ROUTE_ID_TO_NAME = dict(zip(routes_df["route_id"].astype(str), routes_df["route_short_name"].astype(str)))
    
    # Stop-Route Map
    try:
        with zipfile.ZipFile(gtfs_files[0]) as z:
            with z.open("trips.txt") as f:
                trips = pd.read_csv(f, usecols=["route_id", "trip_id"])
            with z.open("stop_times.txt") as f:
                stop_times = pd.read_csv(f, usecols=["trip_id", "stop_id"], dtype={"stop_id": str})
        merged = stop_times.merge(trips, on="trip_id")[["stop_id", "route_id"]].drop_duplicates()
        grouped = merged.groupby("stop_id")["route_id"].apply(set)
        STOP_ROUTE_MAP = grouped.to_dict()
    except Exception as e:
        print(f"âš ï¸ ë§¤í•‘ ì‹¤íŒ¨: {e}")
        STOP_ROUTE_MAP = {}

    # ìºì‹œ ì €ì¥
    with open(meta_cache_path, "wb") as f:
        pickle.dump({
            "stops": STOP_ID_TO_NAME,
            "routes": ROUTE_ID_TO_NAME,
            "stop_route_map": STOP_ROUTE_MAP
        }, f)

# Helper í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ìœ ì§€)
def get_stop_name(stop_id):
    if pd.isna(stop_id): return None
    safe_id = str(stop_id).strip()
    try: safe_id = str(int(float(stop_id))).strip()
    except: pass
    name = STOP_ID_TO_NAME.get(safe_id)
    if not name and len(safe_id) < 5: name = STOP_ID_TO_NAME.get(safe_id.zfill(5))
    return name

def get_route_name(route_id):
    if pd.isna(route_id): return None
    try: safe_id = str(int(float(route_id)))
    except: safe_id = str(route_id)
    return ROUTE_ID_TO_NAME.get(safe_id)

# ============================================================
# 4. ê²½ë¡œ ê³„ì‚° ë° ìƒì„¸í™” (r5py) - ì•ˆì „ì„± ë³´ê°•
# ============================================================
def get_r5py_matrix(nodes, departure_time):
    valid_nodes = [n for n in nodes if n.get("lat") is not None]
    if len(valid_nodes) < 2: return {}

    gdf = gpd.GeoDataFrame(
        valid_nodes,
        geometry=gpd.points_from_xy([n['lng'] for n in valid_nodes], [n['lat'] for n in valid_nodes]),
        crs="EPSG:4326"
    )
    try:
        matrix = TravelTimeMatrix(
            transport_network, origins=gdf, destinations=gdf, departure=departure_time,
            transport_modes=[TransportMode.WALK, TransportMode.TRANSIT]
        )
        r5_travel_times = {}
        for row in matrix.itertuples():
            if not pd.isna(row.travel_time):
                r5_travel_times[(int(row.from_id), int(row.to_id))] = int(row.travel_time)
        return r5_travel_times
    except Exception as e:
        print(f"âš ï¸ í–‰ë ¬ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return {}

def make_cache_key(start_node, end_node, departure_time):
    s_name = start_node.get("name", str(start_node.get("id")))
    e_name = end_node.get("name", str(end_node.get("id")))
    s_coord = f"{start_node.get('lat')}_{start_node.get('lng')}"
    e_coord = f"{end_node.get('lat')}_{end_node.get('lng')}"
    return (f"{s_name}_{s_coord}", f"{e_name}_{e_coord}", int(departure_time.hour))

def get_all_detailed_paths(trip_legs, departure_time):
    if not trip_legs: return {}
    path_map = {}
    origins_list, dests_list = [], []

    for start_node, end_node in trip_legs:
        if start_node['id'] == end_node['id']: continue
        
        cache_key = make_cache_key(start_node, end_node, departure_time)
        if cache_key in DETAILED_PATH_CACHE:
            path_map[(int(start_node['id']), int(end_node['id']))] = DETAILED_PATH_CACHE[cache_key]
            continue

        if start_node.get('lat') is None or end_node.get('lat') is None:
            fallback = {"fastest": [f"ì´ë™(ì¢Œí‘œì—†ìŒ) : {FALLBACK_MOVE_MIN}ë¶„"], "min_transfer": [f"ì´ë™(ì¢Œí‘œì—†ìŒ) : {FALLBACK_MOVE_MIN}ë¶„"]}
            DETAILED_PATH_CACHE[cache_key] = fallback
            path_map[(int(start_node['id']), int(end_node['id']))] = fallback
            continue

        origins_list.append(start_node)
        dests_list.append(end_node)

    if origins_list:
        origins_gdf = gpd.GeoDataFrame(origins_list, geometry=gpd.points_from_xy([n['lng'] for n in origins_list], [n['lat'] for n in origins_list]), crs="EPSG:4326")
        origins_gdf["id"] = [n["id"] for n in origins_list]
        dests_gdf = gpd.GeoDataFrame(dests_list, geometry=gpd.points_from_xy([n['lng'] for n in dests_list], [n['lat'] for n in dests_list]), crs="EPSG:4326")
        dests_gdf["id"] = [n["id"] for n in dests_list]

        try:
            computer = DetailedItineraries(
                transport_network, origins=origins_gdf, destinations=dests_gdf, departure=departure_time,
                transport_modes=[TransportMode.WALK, TransportMode.TRANSIT],
                max_public_transport_rides=MAX_TRANSFERS, max_time=timedelta(minutes=MAX_TRAVEL_TIME_MIN)
            )
        except Exception as e:
            print(f"âš ï¸ DetailedItineraries í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            computer = None

        if computer is not None and not computer.empty:
            mode_col = 'transport_mode' if 'transport_mode' in computer.columns else 'mode'

            def get_val(row, candidates, default=None):
                for c in candidates:
                    if c in row.index and pd.notna(row[c]): return str(row[c]).strip()
                return default

            # [ìµœì¢… ìˆ˜ì •] ì‹œê°„ ì¤‘ë³µ í•©ì‚° ë°©ì§€ë¥¼ ìœ„í•´ ìš”ì•½ ë¡œê·¸ì—ì„œ 'ë¶„' ë‹¨ì–´ ì œê±° ('m'ìœ¼ë¡œ ëŒ€ì²´)
            def parse_route_to_segments_with_congestion(route_df, current_dt):
                segs = []
                total_weighted_min = 0
                
                total_ride_diff = 0
                total_wait_diff = 0

                for _, leg in route_df.iterrows():
                    raw_mode = str(leg[mode_col]).upper() if mode_col in leg.index else ''
                    
                    ride_time = max(1, duration_to_minutes(get_val(leg, ['travel_time', 'duration'], 0)))
                    wait_time = duration_to_minutes(get_val(leg, ['wait_time', 'wait'], 0))
                    
                    f_id = str(get_val(leg, ['start_stop_id', 'from_stop_id']))
                    f_stop_name = get_stop_name(f_id) or "ì •ë¥˜ì¥"
                    
                    f_lat, f_lng = None, None
                    try:
                        if 'geometry' in leg and leg['geometry']:
                            geom = leg['geometry']
                            if hasattr(geom, 'coords'):
                                f_lng, f_lat = geom.coords[0]
                    except Exception:
                        pass
                    
                    t_weight, c_weight, t_emoji, c_emoji = predict_congestion_weights(
                        f_stop_name, current_dt, lat=f_lat, lng=f_lng
                    )
                    
                    final_ride_time = ride_time
                    final_wait_time = wait_time

                    is_subway = any(x in raw_mode for x in ['SUBWAY', 'RAIL', 'METRO'])
                    is_walk = 'WALK' in raw_mode

                    if is_walk:
                        pass
                    else:
                        # ì§€í•˜ì² ì´ ì•„ë‹ˆë©´(ë²„ìŠ¤) ë„ë¡œ í˜¼ì¡ë„ ì ìš©
                        if not is_subway:
                            base_penalty = 1 if t_emoji == 'ğŸ”´' else 0
                            final_ride_time = math.ceil(ride_time * t_weight) + base_penalty
                        
                        if wait_time > 0:
                            final_wait_time = math.ceil(wait_time * c_weight)
                        
                        total_ride_diff += (final_ride_time - ride_time)
                        total_wait_diff += (final_wait_time - wait_time)

                    if final_wait_time > 0 and not is_walk:
                        segs.append(f"ëŒ€ê¸° : {final_wait_time}ë¶„")
                    elif is_walk:
                        # ë„ë³´ì¼ ê²½ìš° ëŒ€ê¸° ì‹œê°„ì„ ê·¸ëƒ¥ ë„ë³´ ì‹œê°„ì— í•©ì³ë²„ë¦¬ëŠ” ë°©ì‹
                        final_ride_time += final_wait_time

                    if is_walk:
                        segs.append(f"ë„ë³´ : {final_ride_time}ë¶„")
                    else:
                        t_id = str(get_val(leg, ['end_stop_id', 'to_stop_id']))
                        t_stop = get_stop_name(t_id) or "ì •ë¥˜ì¥"
                        c_rid = str(get_val(leg, ['route_id']))
                        mode_lbl = "ì§€í•˜ì² " if is_subway else "ë²„ìŠ¤"
                        
                        r_str = get_route_name(c_rid) or 'ëŒ€ì¤‘êµí†µ'
                        if mode_lbl == "ë²„ìŠ¤" and STOP_ROUTE_MAP:
                            common = STOP_ROUTE_MAP.get(f_id, set()).intersection(STOP_ROUTE_MAP.get(t_id, set()))
                            common.add(c_rid)
                            b_names = sorted([n for n in [get_route_name(rid) for rid in common] if n])
                            if b_names: r_str = ", ".join(b_names)

                        segs.append(f"[{mode_lbl}][{r_str}] : {f_stop_name} â†’ {t_stop} : {final_ride_time}ë¶„")

                    total_weighted_min += (final_ride_time + final_wait_time)
                    current_dt += timedelta(minutes=final_ride_time + final_wait_time)
                
                # [ìˆ˜ì •ëœ ì¶œë ¥] 'ë¶„' ê¸€ìë¥¼ í”¼í•´ 'm' ì‚¬ìš© (íŒŒì‹± ë¡œì§ íšŒí”¼)
                if segs:
                    total_delay = int(total_ride_diff + total_wait_diff)
                    if total_delay > 0:
                        # ì˜ˆ: (ì´ 27m ì†Œìš” / ì§€ì—° +9m í¬í•¨) -> 'ë¶„' ê¸€ìê°€ ì—†ì–´ì„œ ê³„ì‚°ì— í¬í•¨ ì•ˆ ë¨
                        summary_str = f" (ì´ {int(total_weighted_min)}m ì†Œìš” / ì§€ì—° +{total_delay}m í¬í•¨)"
                    else:
                        summary_str = f" (ì´ {int(total_weighted_min)}m ì†Œìš”)"
                    segs[-1] += summary_str

                return segs, total_weighted_min

            for (from_id, to_id), group in computer.groupby(['from_id', 'to_id']):
                options_data = []
                for _, opt in group.groupby("option"):
                    raw_time = sum(max(1, duration_to_minutes(get_val(leg, ['travel_time', 'duration'], 0))) for _, leg in opt.iterrows())
                    t_count = sum(1 for _, leg in opt.iterrows() if 'WALK' not in str(leg[mode_col]).upper())
                    options_data.append({"route": opt, "time": raw_time, "transfers": t_count})

                if not options_data: continue

                fastest_opt = min(options_data, key=lambda x: (x['time'], x['transfers']))
                segs_fast, _ = parse_route_to_segments_with_congestion(fastest_opt['route'], departure_time)
                result_entry = {"fastest": segs_fast}

                walk_opts = [o for o in options_data if o['transfers'] == 0]
                best_walk = min(walk_opts, key=lambda x: x['time']) if walk_opts else None

                transit_opts = [o for o in options_data if o['transfers'] > 0]
                transit_opts.sort(key=lambda x: (x['transfers'], x['time']))
                best_transit = transit_opts[0] if transit_opts else None

                winner_opt = best_transit if best_transit else best_walk
                if best_walk and best_transit and best_walk['time'] <= best_transit['time']:
                    winner_opt = best_walk

                if winner_opt:
                    segs_min, _ = parse_route_to_segments_with_congestion(winner_opt['route'], departure_time)
                    result_entry["min_transfer"] = segs_min
                else:
                    result_entry["min_transfer"] = [f"ë„ë³´ : {FALLBACK_MOVE_MIN}ë¶„"]

                cache_key = (int(from_id), int(to_id), int(departure_time.hour))
                DETAILED_PATH_CACHE[cache_key] = result_entry
                path_map[(int(from_id), int(to_id))] = result_entry

    return path_map

# ============================================================
# 5. ë…¸ë“œ ë¹Œë” & ìµœì í™” (OR-Tools)
# ============================================================

def build_fixed_nodes(fixed_events, day_start_dt):
    nodes = []
    BUFFER = 15
    for idx, event in enumerate(fixed_events):
        event_start = parse_time(event["start"]) if event.get("start") else day_start_dt
        event_end = parse_time(event["end"]) if event.get("end") else day_start_dt
        orig_start_min = int((event_start - day_start_dt).total_seconds() / 60)
        orig_end_min = int((event_end - day_start_dt).total_seconds() / 60)

        raw_start_min = orig_start_min - BUFFER
        buffered_start_min = max(0, raw_start_min)
        final_stay = (orig_end_min - orig_start_min) + (orig_start_min - buffered_start_min) + BUFFER

        # ê³ ì •ì¼ì •ì€ ì¢Œí‘œê°€ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ lat/lngëŠ” Noneìœ¼ë¡œ ë‘ 
        nodes.append({
            "name": event.get("title", "ê³ ì •ì¼ì •"), "category": "ê³ ì •ì¼ì •", "lat": None, "lng": None,
            "stay": final_stay, "type": "fixed", "window": (buffered_start_min, buffered_start_min + 10),
            "orig_time_str": f"{event.get('start','00:00')} - {event.get('end','00:00')}"
        })
    return nodes

def build_nodes(places, restaurants, fixed_events, day_start_dt):
    nodes = []
    first_place = places[0] if places else {"lat": 37.5665, "lng": 126.9780}
    nodes.append({"name": "ì‹œì‘ì ", "category": "ì¶œë°œ", "lat": first_place["lat"], "lng": first_place["lng"], "stay": 0, "type": "depot"})

    for p in places:
        nodes.append({"name": p["name"], "category": p["category"], "lat": p.get("lat"), "lng": p.get("lng"), "stay": stay_time_map.get(p["category"], 60), "type": "spot"})

    if restaurants:
        nodes.append({"name": restaurants[0]["name"], "category": "ìŒì‹ì ", "lat": restaurants[0].get("lat"), "lng": restaurants[0].get("lng"), "stay": 70, "type": "lunch"})
        nodes.append({"name": restaurants[1]["name"], "category": "ìŒì‹ì ", "lat": restaurants[1].get("lat"), "lng": restaurants[1].get("lng"), "stay": 70, "type": "dinner"})

    nodes.extend(build_fixed_nodes(fixed_events, day_start_dt))
    return nodes

def build_time_windows(nodes, day_start_dt):
    windows = []
    def get_rel(t_str): return int((parse_time(t_str) - day_start_dt).total_seconds() / 60)
    
    l_s, l_e = get_rel(LUNCH_WINDOW[0]), get_rel(LUNCH_WINDOW[1])
    d_s, d_e = get_rel(DINNER_WINDOW[0]), get_rel(DINNER_WINDOW[1])

    for n in nodes:
        if n["type"] == "lunch": windows.append((l_s, l_e))
        elif n["type"] == "dinner": windows.append((d_s, d_e))
        elif n["type"] == "fixed": windows.append(n["window"])
        else: windows.append((0, 24 * 60))
    return windows

# optimize_day í•¨ìˆ˜ëŠ” ê¸°ì¡´ ë¡œì§ì„ ìœ ì§€í•˜ë˜, get_all_detailed_pathsì—ì„œ ì¢Œí‘œ ì—†ëŠ” êµ¬ê°„ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ë¯€ë¡œ
# ì´í›„ ë¡œì§ì€ ëŒ€ë¶€ë¶„ ê·¸ëŒ€ë¡œ ë™ì‘í•œë‹¤.

def optimize_day(places, restaurants, fixed_events, start_time_str, target_date_str, end_time_str=None):
    # (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    day_start_dt = datetime.strptime(start_time_str, "%H:%M")
    SAFE_GTFS_DATE = target_date_str
    r5_departure_dt = datetime.combine(datetime.strptime(SAFE_GTFS_DATE, "%Y-%m-%d"), datetime.strptime("11:00", "%H:%M").time())
    display_start_dt = datetime.combine(datetime.strptime(target_date_str, "%Y-%m-%d"), day_start_dt.time())
    
    max_horizon_minutes = 24 * 60
    if end_time_str:
        diff = int((datetime.strptime(end_time_str, "%H:%M") - day_start_dt).total_seconds() / 60)
        if diff > 0: max_horizon_minutes = diff

    nodes = build_nodes(places, restaurants, fixed_events, day_start_dt)
    for idx, node in enumerate(nodes): node["id"] = idx
    n = len(nodes)

    r5_travel_times = get_r5py_matrix(nodes, r5_departure_dt)
    time_matrix = [[0]*n for _ in range(n)]
    
    # [ìˆ˜ì • ê³ ë ¤] OR-Tools ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì‹œì—ë„ ì˜ˆì¸¡ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•  ìˆ˜ ìˆìœ¼ë‚˜, 
    # N*N í˜¸ì¶œ ë¹„ìš© ë¬¸ì œë¡œ ì—¬ê¸°ì„œëŠ” í‰ê· ì ì¸ ë³´ìˆ˜ê°’(1.0~1.1)ë§Œ ì ìš©í•˜ê±°ë‚˜ ê¸°ì¡´ ìœ ì§€ë¥¼ ê¶Œì¥.
    # ì¼ë‹¨ ê¸°ì¡´ ë¡œì§ì„ ìœ ì§€í•˜ë˜, ìƒì„¸ ê²½ë¡œ(get_all_detailed_paths)ì—ì„œë§Œ ì •í™•í•œ ë³´ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    for i in range(n):
        for j in range(n):
            if i == j: continue
            val = r5_travel_times.get((i, j))
            if val is None: val = travel_minutes(nodes[i], nodes[j])
            
            if (nodes[i]["type"]=="fixed" or nodes[j]["type"]=="fixed"):
                if not (nodes[i]["type"]=="depot" and nodes[j]["type"]=="fixed"):
                    val = max(val, 30)
            
            time_matrix[i][j] = nodes[i]["stay"] + int(val)

    # (OR-Tools Solver ê¸°ì¡´ê³¼ ë™ì¼)
    manager = pywrapcp.RoutingIndexManager(n, 1, 0)
    routing = pywrapcp.RoutingModel(manager)

    def time_callback(from_idx, to_idx):
        return time_matrix[manager.IndexToNode(from_idx)][manager.IndexToNode(to_idx)]
    
    transit_callback = routing.RegisterTransitCallback(time_callback)
    routing.SetArcCostEvaluatorOfAllVehicles(transit_callback)
    routing.AddDimension(transit_callback, 30, max_horizon_minutes, False, "Time")
    time_dim = routing.GetDimensionOrDie("Time")
    time_windows = build_time_windows(nodes, day_start_dt)
    solver = routing.solver()
    for i, node in enumerate(nodes):
        index = manager.NodeToIndex(i)
        if node["type"] == "depot": continue
        window = time_windows[i]
        if node["type"] == "fixed":
            time_dim.CumulVar(index).SetRange(max(0, window[0]), min(max_horizon_minutes, window[1]))
            continue
        overlap_start, overlap_end = max(0, window[0]), min(max_horizon_minutes, window[1])
        if overlap_start > overlap_end:
            routing.AddDisjunction([index], 0)
            solver.Add(routing.VehicleVar(index) == -1)
        else:
            time_dim.CumulVar(index).SetRange(overlap_start, overlap_end)
            penalty = 1000000 if node["type"] in ["lunch", "dinner"] else 100000
            routing.AddDisjunction([index], penalty)
    search_params = pywrapcp.DefaultRoutingSearchParameters()
    solution = routing.SolveWithParameters(search_params)
    if not solution: return []

    index = routing.Start(0)
    visited_nodes = []
    while not routing.IsEnd(index):
        node_idx = manager.IndexToNode(index)
        nodes[node_idx]['arrival_min'] = solution.Value(time_dim.CumulVar(index))
        visited_nodes.append(nodes[node_idx])
        index = solution.Value(routing.NextVar(index))

    trip_legs = [(visited_nodes[i], visited_nodes[i+1]) for i in range(len(visited_nodes)-1)]
    
    print("ğŸš€ ìƒì„¸ ê²½ë¡œ ê³„ì‚° ì¤‘...")
    # ìƒì„¸ ê²½ë¡œ ê³„ì‚° ì‹œ predict_congestion_weightsê°€ ë‚´ë¶€ì ìœ¼ë¡œ í˜¸ì¶œë¨
    path_map = get_all_detailed_paths(trip_legs, r5_departure_dt) 
    
    # [íƒ€ì„ë¼ì¸ ìƒì„±]
    def build_timeline_by_type(path_type):
        timeline = []
        actual_visits = [n for n in visited_nodes if n["type"] != "depot"]
        cursor = display_start_dt + timedelta(minutes=actual_visits[0]['arrival_min'])

        for i, node in enumerate(actual_visits):
            transit_info = []
            travel_min = 0
            
            if i > 0:
                prev = actual_visits[i-1]
                path_options = path_map.get((prev['id'], node['id']))
                
                if path_options:
                    chosen_path = path_options.get(path_type, path_options.get('fastest', []))
                    transit_info = chosen_path
                    # íŒŒì‹±ëœ ë¬¸ìì—´ì—ì„œ ë¶„ ì¶”ì¶œí•˜ì—¬ í•©ì‚° (ì´ë¯¸ ê°€ì¤‘ì¹˜ ë°˜ì˜ëœ ì‹œê°„ì„)
                    for segment in chosen_path:
                        mins = re.findall(r'(\d+)ë¶„', segment)
                        for m in mins: travel_min += int(m)
                else:
                    # Fallback ì‹œì—ë„ ê°„ë‹¨íˆ ê°€ì¤‘ì¹˜ ì ìš© ê°€ëŠ¥
                    t_w, _ = predict_congestion_weights(prev['name'], cursor)
                    if prev.get('lat') is None or node.get('lat') is None:
                        base_min = FALLBACK_MOVE_MIN
                        travel_min = int(base_min * t_w)
                        transit_info = [f"ì´ë™(ì¢Œí‘œì—†ìŒ) : {travel_min}ë¶„"]
                    else:
                        dist = haversine(prev['lat'], prev['lng'], node['lat'], node['lng'])
                        base_min = int(dist * 15)
                        travel_min = int(base_min * t_w)
                        transit_info = [f"ë„ë³´/ì´ë™ : {travel_min}ë¶„"]
            
            # [í•µì‹¬ ì¶”ê°€] ê° ë°©ë¬¸ ì¥ì†Œ(Spot) ë„ì°© ì‹œì ì˜ í˜¼ì¡ë„ ì˜ˆì¸¡
            _, _, t_stat, c_stat = predict_congestion_weights(
                node["name"], 
                cursor, 
                lat=node.get("lat"), 
                lng=node.get("lng")
            )
            congestion_info = f"Traffic:{t_stat}, Crowd:{c_stat}"

            if node["type"] == "fixed":
                time_parts = node.get("orig_time_str", "00:00 - 00:00").split(" - ")
                start_dt = datetime.strptime(f"{target_date_str} {time_parts[0]}", "%Y-%m-%d %H:%M")
                end_dt = datetime.strptime(f"{target_date_str} {time_parts[1]}", "%Y-%m-%d %H:%M")
                cursor = end_dt
                time_str = node["orig_time_str"]
            else:
                start_dt = cursor + timedelta(minutes=travel_min)
                end_dt = start_dt + timedelta(minutes=node["stay"])
                time_str = f"{start_dt.strftime('%H:%M')} - {end_dt.strftime('%H:%M')}"
                cursor = end_dt

            timeline.append({
                "name": node["name"],
                "category": node["category"],
                "time": time_str,
                "transit_to_here": transit_info,
                "congestion": congestion_info
            })
        return timeline

    return {
        "fastest_version": build_timeline_by_type("fastest"),
        "min_transfer_version": build_timeline_by_type("min_transfer")
    }

# ============================================================
# 6. ë©”ì¸ ì‹¤í–‰ë¶€ (í†µí•©)
# ============================================================
if __name__ == "__main__":
    # 1. ì—‘ì…€ ë° ê¸°ë³¸ ì •ë³´ ë¡œë“œ
    print("ğŸ“‚ ì¥ì†Œ ë°ì´í„° ë¡œë“œ ì¤‘...")
    try:
        df = pd.read_excel("./data/place_ì „ì²´_í†µí•©.xlsx")
        df.columns = ["category", "name", "place_id", "area", "lat", "lng"]
    except FileNotFoundError:
        print("âŒ 'places_ì „ì²´_í†µí•©.xlsx' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    SEOUL_GU_COORDS = {
    "ê°•ë‚¨êµ¬": {"lat": 37.514575, "lon": 127.0495556},
    "ê°•ë™êµ¬": {"lat": 37.52736667, "lon": 127.1258639},
    "ê°•ë¶êµ¬": {"lat": 37.63695556, "lon": 127.0277194},
    "ê°•ì„œêµ¬": {"lat": 37.54815556, "lon": 126.851675},
    "ê´€ì•…êµ¬": {"lat": 37.47538611, "lon": 126.9538444},
    "ê´‘ì§„êµ¬": {"lat": 37.53573889, "lon": 127.0845333},
    "êµ¬ë¡œêµ¬": {"lat": 37.49265, "lon": 126.8895972},
    "ê¸ˆì²œêµ¬": {"lat": 37.44910833, "lon": 126.9041972},
    "ë…¸ì›êµ¬": {"lat": 37.65146111, "lon": 127.0583889},
    "ë„ë´‰êµ¬": {"lat": 37.66583333, "lon": 127.0495222},
    "ë™ëŒ€ë¬¸êµ¬": {"lat": 37.571625, "lon": 127.0421417},
    "ë™ì‘êµ¬": {"lat": 37.50965556, "lon": 126.941575},
    "ë§ˆí¬êµ¬": {"lat": 37.56070556, "lon": 126.9105306},
    "ì„œëŒ€ë¬¸êµ¬": {"lat": 37.57636667, "lon": 126.9388972},
    "ì„œì´ˆêµ¬": {"lat": 37.48078611, "lon": 127.0348111},
    "ì„±ë™êµ¬": {"lat": 37.56061111, "lon": 127.039},
    "ì„±ë¶êµ¬": {"lat": 37.58638333, "lon": 127.0203333},
    "ì†¡íŒŒêµ¬": {"lat": 37.51175556, "lon": 127.1079306},
    "ì–‘ì²œêµ¬": {"lat": 37.51423056, "lon": 126.8687083},
    "ì˜ë“±í¬êµ¬": {"lat": 37.52361111, "lon": 126.8983417},
    "ìš©ì‚°êµ¬": {"lat": 37.53609444, "lon": 126.9675222},
    "ì€í‰êµ¬": {"lat": 37.59996944, "lon": 126.9312417},
    "ì¢…ë¡œêµ¬": {"lat": 37.57037778, "lon": 126.9816417},
    "ì¤‘êµ¬": {"lat": 37.56100278, "lon": 126.9996417},
    "ì¤‘ë‘êµ¬": {"lat": 37.60380556, "lon": 127.0947778},
    }

    area = input("ì—¬í–‰í•  ì§€ì—­ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì¢…ë¡œêµ¬): ")

    if area not in SEOUL_GU_COORDS:
        raise ValueError("ì„œìš¸ êµ¬ ì´ë¦„ì´ ì•„ë‹™ë‹ˆë‹¤.")
    
    center_lat = SEOUL_GU_COORDS[area]["lat"]
    center_lon = SEOUL_GU_COORDS[area]["lon"]

    df["distance_km"] = df.apply(lambda r: haversine(center_lat, center_lon, r["lat"], r["lng"]), axis=1)
    RADIUS_KM = 6
    
    # 2. ì¥ì†Œ í•„í„°ë§
    area_mask = df[df["distance_km"] <= RADIUS_KM].copy()
    print(f"\nğŸ“ {area} ì¤‘ì‹¬ ë°˜ê²½ {RADIUS_KM}km ì´ë‚´ ì¥ì†Œ ìˆ˜: {len(area_mask)}")
    
    dist_mask = df["distance_km"] <= RADIUS_KM

    filtered_spot = df[dist_mask & (df["category"] != "ìŒì‹ì ") & (df["category"] != "ìˆ™ë°•")][["name", "lat", "lng"]]

    filtered_restaurant = df[dist_mask & (df["category"] == "ìŒì‹ì ")][["name", "lat", "lng"]]

    filtered_accom = df[dist_mask & (df["category"] == "ìˆ™ë°•")][["name", "lat", "lng"]]

    places = filtered_spot.to_dict(orient="records")
    print(len(places), "ê°œì˜ ê´€ê´‘ì§€ê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")

    restaurants = filtered_restaurant.to_dict(orient="records")
    print(len(restaurants), "ê°œì˜ ìŒì‹ì ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")

    accommodations = filtered_accom.to_dict(orient="records")
    print(len(accommodations), "ê°œì˜ ìˆ™ë°• ì‹œì„¤ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 3. ë‚ ì§œ ì…ë ¥
    start_date = input("ì—¬í–‰ ì‹œì‘ ì¼ì (ì˜ˆ: 2026-01-20): ")
    end_date = input("ì—¬í–‰ ì¢…ë£Œ ì¼ì (ì˜ˆ: 2026-01-25): ")
    
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")
    days = (end - start).days + 1
    print(f"ì´ ì—¬í–‰ ì¼ìˆ˜: {days}ì¼")

    # # 4. Gemini API í˜¸ì¶œ (1ì°¨ ê³„íš ìƒì„±)
    # schema = """
    # {
    #   "plans": {
    #     "day1": {
    #       "route": [
    #         {"name": "...", "category": "...", "lat": 0.0, "lng": 0.0}
    #       ],
    #       "restaurants": [
    #         {"name": "...", "category": "ìŒì‹ì ", "lat": 0.0, "lng": 0.0}
    #       ],
    #       "accommodations": [
    #         {"name": "...", "category": "ìˆ™ë°•", "lat": 0.0, "lng": 0.0}
    #       ]
    #     }
    #   }
    # }
    # """
    
    # system_prompt = f"""
    # ë„ˆëŠ” 'ì„œìš¸ ì—¬í–‰ ì¥ì†Œ ì¶”ì²œ ì „ë¬¸ê°€'ì´ë‹¤. ë°˜ë“œì‹œ ì œê³µëœ ë°ì´í„°ë§Œì„ ì‚¬ìš©í•˜ì—¬ ê³„íšì„ ì„¸ìš´ë‹¤.
    # {schema}
    # [ì ˆëŒ€ ê·œì¹™]
    # 1. ëª¨ë“  ì¥ì†Œì˜ ì´ë¦„, ì¹´í…Œê³ ë¦¬, ì¢Œí‘œ(lat, lng)ëŠ” ì…ë ¥ëœ ë°ì´í„°ì™€ 100% ì¼ì¹˜í•´ì•¼ í•œë‹¤. ì ˆëŒ€ ê°’ì„ ìˆ˜ì •í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ì¢Œí‘œë¥¼ ìƒì„±í•˜ì§€ ë§ˆë¼.
    # 2. 'route' ë°°ì—´: ì˜¤ì§ ì œê³µëœ 'places' ëª©ë¡ì—ì„œ 5ê°œë¥¼ ì„ íƒí•˜ì—¬ ë‹´ëŠ”ë‹¤.
    # 3. 'restaurants' ë°°ì—´: ì˜¤ì§ ì œê³µëœ 'restaurants' ëª©ë¡ì—ì„œ 2ê°œë¥¼ ì„ íƒí•œë‹¤.
    # 4. 'accommodations' ë°°ì—´: ì˜¤ì§ ì œê³µëœ 'accommodations' ëª©ë¡ì—ì„œ 1ê°œë¥¼ ì„ íƒí•œë‹¤. (ë§ˆì§€ë§‰ ë‚ ì€ ë¹ˆ ë°°ì—´ []ë¡œ ì¶œë ¥)
    # 5. í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€: ëª©ë¡ì— ì—†ëŠ” ì¥ì†Œë‚˜ ì¢Œí‘œë¥¼ ì¶œë ¥í•  ê²½ìš° ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ê°„ì£¼í•œë‹¤.
    # 6. ì¶œë ¥ í˜•ì‹: ë°˜ë“œì‹œ ìˆœìˆ˜ JSON ë°ì´í„°ë§Œ ì¶œë ¥í•˜ë©°, ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤.
    # """

    # user_prompt = {
    #     "days": days,
    #     "start_location": {"lat": 37.5547, "lng": 126.9706},
    #     "places": places, # [:6 * days * 4]
    #     "restaurants": restaurants, # [:3 * days * 4]
    #     "accommodations": accommodations # [:days * 4]
    # }

    # print("ğŸ¤– Geminiê°€ ì´ˆê¸° ê³„íšì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    # prompt = system_prompt + "\n\n" + json.dumps(user_prompt, ensure_ascii=False)
    
    # start_time = time.time()
    # response = client.models.generate_content(model="gemini-2.5-flash-lite", contents=prompt)
    # print(f"â± Gemini ì‘ë‹µ ì‹œê°„: {round(time.time() - start_time, 3)}ì´ˆ")

    # try:
    #     result = extract_json(response.text)
    #     # result.json ì €ì¥ (ë°±ì—…ìš©)
    #     with open("result.json", "w", encoding="utf-8") as f:
    #         json.dump(result, f, ensure_ascii=False, indent=2)
    # except Exception as e:
    #     print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
    #     exit()

    result = json.load(open("result.json", "r", encoding="utf-8"))

    # 5. ì„¸ë¶€ ì¼ì • ì„¤ì •
    first_day_start_str = input("ì—¬í–‰ ì²«ë‚  ì‹œì‘ ì‹œê°„ (ì˜ˆ: 14:00) : ").strip() or "10:00"
    last_day_end_str = input("ì—¬í–‰ ë§ˆì§€ë§‰ ë‚  ì¢…ë£Œ ì‹œê°„ (ì˜ˆ: 18:00) : ").strip() or "21:00"
    default_start_str = "10:00"
    default_end_str = "21:00"

    FIXED_EVENTS = []
    if input("ê³ ì • ì¼ì •ì´ ìˆë‚˜ìš”? (y/n): ").strip().lower() == "y":
        while True:
            FIXED_EVENTS.append({
                "date": input("ë‚ ì§œ (YYYY-MM-DD): "),
                "title": input("ì œëª©: "),
                "start": input("ì‹œì‘(HH:MM): "),
                "end": input("ì¢…ë£Œ(HH:MM): ")
            })
            if input("ë” ì¶”ê°€í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() != "y": break

    # 6. ìµœì í™” ì‹¤í–‰ (Day loop)
    plans = result["plans"]
    day_keys = list(plans.keys())

    print(f"\nğŸš€ ë³‘ë ¬ ìµœì í™” ì‹œì‘: {len(day_keys)}ì¼ì¹˜ ì¼ì •ì„ ë™ì‹œì— ê³„ì‚°í•©ë‹ˆë‹¤.")
    start_total_opt = time.time()

    # [ë‚´ë¶€ í•¨ìˆ˜] ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë˜í¼ í•¨ìˆ˜
    def process_day_wrapper(args):
        day_key, date_obj, is_first, is_last = args
        
        todays_start = first_day_start_str if is_first else default_start_str
        todays_end = last_day_end_str if is_last else default_end_str
        current_date_str = date_obj.strftime("%Y-%m-%d")
        
        print(f"   â–¶ {day_key} ìµœì í™” ì‹œì‘...")
        
        # ì‹¤ì œ ìµœì í™” ìˆ˜í–‰
        day_res = optimize_day(
            places=plans[day_key]["route"],
            restaurants=plans[day_key]["restaurants"],
            fixed_events=get_fixed_events_for_day(FIXED_EVENTS, current_date_str),
            start_time_str=todays_start,
            target_date_str=current_date_str,
            end_time_str=todays_end
        )
        return day_key, day_res

    # 6-1. ë³‘ë ¬ ì‹¤í–‰ ì¸ì(Task) ì¤€ë¹„
    tasks = []
    curr = start
    for i, day_key in enumerate(day_keys):
        tasks.append((day_key, curr, i==0, i==len(day_keys)-1))
        curr += timedelta(days=1)

    # 6-2. ThreadPoolExecutorë¡œ ë³‘ë ¬ ì‹¤í–‰
    processed_results = {}
    
    if available_cores >= days * 2:
        available_cores = days * 2
    else:
        available_cores = available_cores - 2

    print(f"âš™ï¸ ìµœëŒ€ {available_cores}ê°œ ì½”ì–´ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ì¤‘...")

    with ThreadPoolExecutor(max_workers=available_cores) as executor:
        for day_key, day_res in executor.map(process_day_wrapper, tasks):
            processed_results[day_key] = day_res
            print(f"   âœ… {day_key} ì™„ë£Œ")

    print(f"â± ì „ì²´ ìµœì í™” ì™„ë£Œ: {round(time.time() - start_total_opt, 2)}ì´ˆ")
    
    # 3. ê²°ê³¼ ì·¨í•© ë° í™”ë©´ ì¶œë ¥
    curr = start
    for i, day_key in enumerate(day_keys):
        # ê²°ê³¼ ì €ì¥
        result["plans"][day_key]["timelines"] = processed_results[day_key]
        day_results = processed_results[day_key]
        
        print(f"\nğŸ“… {day_key} ({curr.strftime('%Y-%m-%d')})")

        # ë‘ ê°€ì§€ ë²„ì „(ìµœë‹¨ ì‹œê°„, ìµœì†Œ í™˜ìŠ¹) ëª¨ë‘ ì¶œë ¥
        for ver_key, label in [("fastest_version", "ìµœë‹¨ ì‹œê°„"), ("min_transfer_version", "ìµœì†Œ í™˜ìŠ¹")]:
            timeline = day_results[ver_key]
            
            separator = "-" * 60
            print(f"\n[{label} ê¸°ì¤€ ì¼ì •] {day_key}")
            print(separator)

            for t in timeline:
                if t.get('transit_to_here'):
                    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ê²½ë¡œë¥¼ í™”ì‚´í‘œë¡œ ì—°ê²°í•˜ì—¬ ì¶œë ¥
                    path_str = " -> ".join([s for s in t['transit_to_here']])
                    print(f"  [TRANSIT] {path_str}")
                congestion_log = t.get('congestion', 'N/A')
                print(f"  [{t['time']}] {t['name']} ({t['category']}) - {congestion_log}")
            
            print(separator)

        # ë‚ ì§œ ì¹´ìš´í„° ì¦ê°€
        curr += timedelta(days=1)

    # 7. ëª¨ë“  ë£¨í”„ê°€ ëë‚œ í›„ ìµœì¢… íŒŒì¼ ì €ì¥ (ë£¨í”„ ì™¸ë¶€)
    with open("result_timeline_congestion.json", "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
        print("\nì „ì²´ ì¼ì •ì´ 'result_timeline_congestion.json' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")