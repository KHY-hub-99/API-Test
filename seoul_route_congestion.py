import os, pickle, re, time, math, json, zipfile, joblib
import multiprocessing

available_cores = multiprocessing.cpu_count()
JAVA_PARALLELISM = max(2, available_cores // 2)
print(f"âš™ï¸  Java ë‚´ë¶€ ë³‘ë ¬ì„± ì„¤ì •: {JAVA_PARALLELISM}ê°œ")
os.environ["JAVA_HOME"] = r"C:\Program Files\Java\jdk-21.0.10"
os.environ["JAVA_OPTS"] = f"-Xmx8G -Djava.util.concurrent.ForkJoinPool.common.parallelism={JAVA_PARALLELISM}"

from google import genai
import pandas as pd
import geopandas as gpd
from datetime import datetime, timedelta
from dotenv import load_dotenv
from ortools.constraint_solver import routing_enums_pb2, pywrapcp
from r5py import TransportNetwork, TravelTimeMatrix, DetailedItineraries, TransportMode
from concurrent.futures import ThreadPoolExecutor

# ============================================================
# [NEW] 0. í˜¼ì¡ë„ ëª¨ë¸ ë° ì„¤ì • ë¡œë“œ
# ============================================================
print("ğŸ§  í˜¼ì¡ë„ ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ ì¤‘...")
try:
    CONGESTION_MODEL = joblib.load('./model/congestion_model_latlon.pkl')
    # ì´ì „ ë‹¨ê³„ì—ì„œ í•œêµ­ì–´ ì»¬ëŸ¼ëª…ìœ¼ë¡œ í•™ìŠµí–ˆìœ¼ë¯€ë¡œ ìˆœì„œë¥¼ ë§ì¶°ì¤ë‹ˆë‹¤.
    # ['month', 'day', 'hour', 'dayofweek', 'is_holiday', 'is_weekend', 'ìœ„ë„', 'ê²½ë„']
    print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    CONGESTION_MODEL = None

# ê³µíœ´ì¼ ì •ì˜ (ëª¨ë¸ í•™ìŠµë•Œì™€ ë™ì¼í•˜ê²Œ)
KOREAN_HOLIDAYS_2026 = [
    '20260101', # ì‹ ì • (ëª©)
    '20260216', '20260217', '20260218', # ì„¤ë‚  ì—°íœ´ (ì›”, í™”, ìˆ˜)
    '20260301', # ì‚¼ì¼ì ˆ (ì¼)
    '20260302', # ì‚¼ì¼ì ˆ ëŒ€ì²´ê³µíœ´ì¼ (ì›”)
    '20260505', # ì–´ë¦°ì´ë‚  (í™”)
    '20260524', # ë¶€ì²˜ë‹˜ì˜¤ì‹ ë‚  (ì¼)
    '20260525', # ë¶€ì²˜ë‹˜ì˜¤ì‹ ë‚  ëŒ€ì²´ê³µíœ´ì¼ (ì›”)
    '20260606', # í˜„ì¶©ì¼ (í† )
    '20260608', # í˜„ì¶©ì¼ ëŒ€ì²´ê³µíœ´ì¼ (ì›”) - *ê´€ê³µì„œ ê³µíœ´ì¼ ê·œì •ì— ë”°ë¼ ì ìš© ì˜ˆìƒ
    '20260815', # ê´‘ë³µì ˆ (í† )
    '20260817', # ê´‘ë³µì ˆ ëŒ€ì²´ê³µíœ´ì¼ (ì›”)
    '20260924', '20260925', '20260926', # ì¶”ì„ ì—°íœ´ (ëª©, ê¸ˆ, í† )
    '20261003', # ê°œì²œì ˆ (í† )
    '20261005', # ê°œì²œì ˆ ëŒ€ì²´ê³µíœ´ì¼ (ì›”)
    '20261009', # í•œê¸€ë‚  (ê¸ˆ)
    '20261225'  # í¬ë¦¬ìŠ¤ë§ˆìŠ¤ (ê¸ˆ)
]

def get_congestion_level(lat, lng, dt):
    """
    ìœ„ì¹˜ì™€ ì‹œê°„ì„ ë°›ì•„ í˜¼ì¡ë„(0:Low, 1:Med, 2:High)ë¥¼ ë°˜í™˜
    """
    if CONGESTION_MODEL is None or lat is None or lng is None:
        return 0 
    
    # íŒŒìƒ ë³€ìˆ˜ ìƒì„±
    month = dt.month
    day = dt.day
    hour = dt.hour
    
    # [ìˆ˜ì •] datetime ê°ì²´ëŠ” .dayofweek ì†ì„±ì´ ì—†ìœ¼ë¯€ë¡œ .weekday() ë©”ì„œë“œ ì‚¬ìš©
    # ì›”ìš”ì¼=0, ... ì¼ìš”ì¼=6 (Pandas dayofweekì™€ ë™ì¼)
    dayofweek = dt.weekday() 
    
    date_str = dt.strftime('%Y%m%d')
    
    # [ìˆ˜ì •] 2026ë…„ ê³µíœ´ì¼ ë¦¬ìŠ¤íŠ¸ ì°¸ì¡° í™•ì¸
    is_holiday = 1 if date_str in KOREAN_HOLIDAYS_2026 else 0
    is_weekend = 1 if dayofweek >= 5 else 0
    
    # ì…ë ¥ ë°ì´í„° í”„ë ˆì„ ìƒì„±
    input_vector = pd.DataFrame([[
        month, day, hour, dayofweek, is_holiday, is_weekend, lat, lng
    ]], columns=['month', 'day', 'hour', 'dayofweek', 'is_holiday', 'is_weekend', 'ìœ„ë„', 'ê²½ë„'])
    
    return CONGESTION_MODEL.predict(input_vector)[0]

def get_stay_weight(level):
    """
    í˜¼ì¡ë„ ë“±ê¸‰ì— ë”°ë¥¸ ì‹œê°„ ê°€ì¤‘ì¹˜ ë°˜í™˜
    0 (Low) -> 1.0 (ë³€í™” ì—†ìŒ)
    1 (Med) -> 1.1 (10% ì¦ê°€)
    2 (High) -> 1.3 (30% ì¦ê°€)
    """
    if level == 2: return 1.3
    elif level == 1: return 1.1
    else: return 1.0

def get_wait_weight(level):
    """
    ëŒ€ê¸° ì‹œê°„ ì „ìš© ê°€ì¤‘ì¹˜
    0 (Low) -> 1.0 (ë³€í™” ì—†ìŒ)
    1 (Med) -> 1.5 (50% ì¦ê°€)
    2 (High) -> 2.0 (2ë°° ì¦ê°€)
    """
    if level == 2: return 2.0
    elif level == 1: return 1.5
    else: return 1.0

# ============================================================
# 1. í™˜ê²½ ì„¤ì • ë° ì „ì—­ ìƒìˆ˜
# ============================================================ 
# API í‚¤ ì„¤ì •
load_dotenv()
API_KEY = os.getenv("API_KEY")
client = genai.Client(api_key=API_KEY)

# ìºì‹œ ì €ì¥ì†Œ
DETAILED_PATH_CACHE = {}

# í´ë°±(ì¢Œí‘œ ì—†ëŠ” ê²½ìš°) ì´ë™ ì‹œê°„ ì„¤ì •(ë¶„)
FALLBACK_MOVE_MIN = 30

# ë„ë³´ ì´ë™ ì œí•œ (km -> ë¶„ í™˜ì‚° ê¸°ì¤€ ë“±)
WALK_ONLY_THRESHOLD_MIN = 12   
WALK_ONLY_THRESHOLD_MAX = 18   

MAX_TRANSFERS = 2
MAX_TRAVEL_TIME_MIN = 90

# ì‹œê°„ ìœˆë„ìš° ì„¤ì •
LUNCH_WINDOW = ("11:20", "13:20")
DINNER_WINDOW = ("17:40", "19:30")

# ì¥ì†Œë³„ ì²´ë¥˜ ì‹œê°„
stay_time_map = {
    "ê´€ê´‘ì§€": 90, "ì¹´í˜": 50, "ìŒì‹ì ": 70, 
    "ë°•ë¬¼ê´€": 120, "ê³µì›": 60, "ì‹œì¥": 80, "ìˆ™ë°•": 0
}

# ë°ì´í„° íŒŒì¼ ê²½ë¡œ
osm_file = "./data/seoul_osm_v.pbf"
gtfs_files = ["./data/seoul_area_gtfs.zip"]

# ì„œìš¸ êµ¬ë³„ ì¤‘ì‹¬ ì¢Œí‘œ
SEOUL_GU_COORDS = {
"ê°•ë‚¨êµ¬": {"lat": 37.514575, "lon": 127.0495556},
"ê°•ë™êµ¬": {"lat": 37.52736667, "lon": 127.1258639},
"ê°•ë¶êµ¬": {"lat": 37.63695556, "lon": 127.0277194},
"ê°•ì„œêµ¬": {"lat": 37.54815556, "lon": 126.851675},
"ê´€ì•…êµ¬": {"lat": 37.47538611, "lon": 126.9538444},
"ê´‘ì§„êµ¬": {"lat": 37.53573889, "lon": 127.0845333},
"êµ¬ë¡œêµ¬": {"lat": 37.49265, "lon": 126.8895972},
"ê¸ˆì²œêµ¬": {"lat": 37.44910833, "lon": 126.9041972},
"ë…¸ì›êµ¬": {"lat": 37.65146111, "lon": 127.0583889},
"ë„ë´‰êµ¬": {"lat": 37.66583333, "lon": 127.0495222},
"ë™ëŒ€ë¬¸êµ¬": {"lat": 37.571625, "lon": 127.0421417},
"ë™ì‘êµ¬": {"lat": 37.50965556, "lon": 126.941575},
"ë§ˆí¬êµ¬": {"lat": 37.56070556, "lon": 126.9105306},
"ì„œëŒ€ë¬¸êµ¬": {"lat": 37.57636667, "lon": 126.9388972},
"ì„œì´ˆêµ¬": {"lat": 37.48078611, "lon": 127.0348111},
"ì„±ë™êµ¬": {"lat": 37.56061111, "lon": 127.039},
"ì„±ë¶êµ¬": {"lat": 37.58638333, "lon": 127.0203333},
"ì†¡íŒŒêµ¬": {"lat": 37.51175556, "lon": 127.1079306},
"ì–‘ì²œêµ¬": {"lat": 37.51423056, "lon": 126.8687083},
"ì˜ë“±í¬êµ¬": {"lat": 37.52361111, "lon": 126.8983417},
"ìš©ì‚°êµ¬": {"lat": 37.53609444, "lon": 126.9675222},
"ì€í‰êµ¬": {"lat": 37.59996944, "lon": 126.9312417},
"ì¢…ë¡œêµ¬": {"lat": 37.57037778, "lon": 126.9816417},
"ì¤‘êµ¬": {"lat": 37.56100278, "lon": 126.9996417},
"ì¤‘ë‘êµ¬": {"lat": 37.60380556, "lon": 127.0947778},
}

# ============================================================
# 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================
def parse_time(t):
    return datetime.strptime(t, "%H:%M")

def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # km
    phi1, phi2 = math.radians(lat1), math.radians(lat2)
    dphi = math.radians(lat2 - lat1)
    dlambda = math.radians(lon2 - lon1)
    a = math.sin(dphi / 2) ** 2 + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2) ** 2
    return 2 * R * math.atan2(math.sqrt(a), math.sqrt(1 - a))

def approx_walk_minutes(start, end):
    # start/endê°€ ì¢Œí‘œ ì—†ìŒ(None)ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „ ì²˜ë¦¬
    if not start or not end or start.get("lat") is None or end.get("lat") is None:
        return FALLBACK_MOVE_MIN
    dist_km = haversine(start["lat"], start["lng"], end["lat"], end["lng"])
    return dist_km * 15

def dynamic_walk_threshold(dist_km):
    if dist_km < 0.6: return WALK_ONLY_THRESHOLD_MAX
    elif dist_km < 1.2: return 15
    else: return WALK_ONLY_THRESHOLD_MIN

def travel_minutes(p1, p2):
    # ì¢Œí‘œê°€ ì—†ìœ¼ë©´ 0ì„ ë°˜í™˜(ìƒìœ„ ë¡œì§ì—ì„œ ê³ ì •ì¼ì • ë³´ì •ìœ¼ë¡œ ìµœì†Œ ì‹œê°„ ì ìš©ë¨)
    if p1 is None or p2 is None or p1.get("lat") is None or p2.get("lat") is None: return 0
    dist = haversine(p1["lat"], p1["lng"], p2["lat"], p2["lng"])
    return int(dist / 30 * 60)

def get_fixed_events_for_day(fixed_events, target_date):
    return [e for e in fixed_events if e["date"] == target_date]

def duration_to_minutes(val):
    if val is None or pd.isna(val): return 0
    if hasattr(val, "total_seconds"): return int(val.total_seconds() / 60)
    if isinstance(val, str) and "day" in val:
        try: return int(pd.to_timedelta(val).total_seconds() / 60)
        except: return 0
    try: return int(float(val))
    except: return 0

def extract_json(text):
    if not text:
        raise ValueError("Gemini ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    text = text.strip()
    if text.startswith("```"):
        text = text.split("```")[1]
        if text.startswith("json"):
            text = text[4:]
    start = text.find("{")
    end = text.rfind("}") + 1
    if start == -1 or end == -1:
        raise ValueError("JSON íŒŒì‹± ì‹¤íŒ¨:\n" + text)
    return json.loads(text[start:end])

# ============================================================
# 3. êµí†µ ë°ì´í„° ë¡œë“œ (GTFS & OSM)
# ============================================================
pickle_path = "./data/seoul_tn_cached.pkl"
if os.path.exists(pickle_path):
    print(f"ğŸ“¦ TransportNetwork ìºì‹œ ë¡œë“œ ì¤‘...")
    try:
        # ì•ˆì „í•œ ë¡œë”©ì„ ìœ„í•´ í´ë˜ìŠ¤ ë©”ì„œë“œ ëŒ€ì‹  ì§ì ‘ ë¡œë“œ ì‹œë„
        with open(pickle_path, 'rb') as f:
            transport_network = pickle.load(f)
    except Exception:
        # êµ¬ë²„ì „ pickle í˜¸í™˜ ë¬¸ì œ ì‹œ ì¬ìƒì„±
        transport_network = TransportNetwork.__new__(TransportNetwork)
        transport_network._transport_network = TransportNetwork._load_pickled_transport_network(transport_network, pickle_path)
else:
    print("ğŸš€ TransportNetwork ìƒì„± ì¤‘ (ìµœì´ˆ 1íšŒ)...")
    transport_network = TransportNetwork(osm_file, gtfs_files)
    try:
        # ìµœì‹  r5py ë°©ì‹ ì €ì¥
        transport_network.save(pickle_path)
    except:
        pass

meta_cache_path = "./data/metadata_cache_v2.pkl" # íŒŒì¼ëª… v2ë¡œ ë³€ê²½ (ìºì‹œ ê°±ì‹ ì„ ìœ„í•´)

STOP_COORDS = {} # ì „ì—­ ë³€ìˆ˜

if os.path.exists(meta_cache_path):
    print("âš¡ ë©”íƒ€ë°ì´í„°(ì¢Œí‘œí¬í•¨) ìºì‹œ ë¡œë“œ ì¤‘...")
    with open(meta_cache_path, "rb") as f:
        meta_data = pickle.load(f)
        STOP_ID_TO_NAME = meta_data["stops"]
        ROUTE_ID_TO_NAME = meta_data["routes"]
        STOP_ROUTE_MAP = meta_data["stop_route_map"]
        STOP_COORDS = meta_data["coords"] # [NEW] ì¢Œí‘œ ë¡œë“œ
else:
    print("ğŸ¢ ë©”íƒ€ë°ì´í„° ìƒì„± ì¤‘ (ì¢Œí‘œ í¬í•¨)...")
    # Stops
    with zipfile.ZipFile(gtfs_files[0]) as z:
        with z.open("stops.txt") as f:
            # [NEW] stop_lat, stop_lon ì»¬ëŸ¼ ì¶”ê°€ ë¡œë“œ
            stops_df = pd.read_csv(f, dtype={'stop_id': str}, usecols=['stop_id', 'stop_name', 'stop_lat', 'stop_lon'])
    
    STOP_ID_TO_NAME = {str(row['stop_id']).strip(): str(row['stop_name']).strip() for _, row in stops_df.iterrows()}
    
    # [NEW] ì •ë¥˜ì¥ ID -> ì¢Œí‘œ ë§¤í•‘ ìƒì„±
    for _, row in stops_df.iterrows():
        s_id = str(row['stop_id']).strip()
        STOP_COORDS[s_id] = {'lat': row['stop_lat'], 'lng': row['stop_lon']}
    
    # Routes (ê¸°ì¡´ ë™ì¼)
    with zipfile.ZipFile(gtfs_files[0]) as z:
        with z.open("routes.txt") as f:
            routes_df = pd.read_csv(f)
    ROUTE_ID_TO_NAME = dict(zip(routes_df["route_id"].astype(str), routes_df["route_short_name"].astype(str)))
    
    # Stop-Route Map (ê¸°ì¡´ ë™ì¼)
    try:
        with zipfile.ZipFile(gtfs_files[0]) as z:
            with z.open("trips.txt") as f:
                trips = pd.read_csv(f, usecols=["route_id", "trip_id"])
            with z.open("stop_times.txt") as f:
                stop_times = pd.read_csv(f, usecols=["trip_id", "stop_id"], dtype={"stop_id": str})
        merged = stop_times.merge(trips, on="trip_id")[["stop_id", "route_id"]].drop_duplicates()
        grouped = merged.groupby("stop_id")["route_id"].apply(set)
        STOP_ROUTE_MAP = grouped.to_dict()
    except Exception as e:
        STOP_ROUTE_MAP = {}

    # ìºì‹œ ì €ì¥
    with open(meta_cache_path, "wb") as f:
        pickle.dump({
            "stops": STOP_ID_TO_NAME,
            "routes": ROUTE_ID_TO_NAME,
            "stop_route_map": STOP_ROUTE_MAP,
            "coords": STOP_COORDS # [NEW] ì €ì¥
        }, f)

def get_stop_name(stop_id):
    if pd.isna(stop_id): return None
    safe_id = str(stop_id).strip()
    try: safe_id = str(int(float(stop_id))).strip()
    except: pass
    name = STOP_ID_TO_NAME.get(safe_id)
    if not name and len(safe_id) < 5: name = STOP_ID_TO_NAME.get(safe_id.zfill(5))
    return name

def get_route_name(route_id):
    if pd.isna(route_id): return None
    try: safe_id = str(int(float(route_id)))
    except: safe_id = str(route_id)
    return ROUTE_ID_TO_NAME.get(safe_id)

# ============================================================
# 4. ê²½ë¡œ ê³„ì‚° ë° ìƒì„¸í™” (r5py) - ì•ˆì „ì„± ë³´ê°•
# ============================================================

def get_r5py_matrix(nodes, departure_time):
    valid_nodes = [n for n in nodes if n.get("lat") is not None]
    if len(valid_nodes) < 2: return {}

    gdf = gpd.GeoDataFrame(
        valid_nodes,
        geometry=gpd.points_from_xy([n['lng'] for n in valid_nodes], [n['lat'] for n in valid_nodes]),
        crs="EPSG:4326"
    )

    try:
        matrix = TravelTimeMatrix(
            transport_network, origins=gdf, destinations=gdf, departure=departure_time,
            transport_modes=[TransportMode.WALK, TransportMode.TRANSIT]
        )
        r5_travel_times = {}
        for row in matrix.itertuples():
            if not pd.isna(row.travel_time):
                r5_travel_times[(int(row.from_id), int(row.to_id))] = int(row.travel_time)
        return r5_travel_times
    except Exception as e:
        print(f"âš ï¸ í–‰ë ¬ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return {}


def make_cache_key(start_node, end_node, departure_time):
    """ìºì‹œ í‚¤ ìƒì„±ì„ ì¼ê´€ì„± ìˆê²Œ ê´€ë¦¬"""
    s_id = start_node.get("id")
    e_id = end_node.get("id")
    # ì¢Œí‘œ ê¸°ë°˜ ìœ ë‹ˆí¬ì„± í™•ë³´ë¥¼ ìœ„í•´ IDì™€ ì‹œê°„ëŒ€ ì¡°í•©
    return (s_id, e_id, departure_time.hour)


def get_all_detailed_paths(trip_legs, departure_time):
    if not trip_legs: return {}
    path_map = {}
    origins_list, dests_list = [], []

    # 1) ìš”ì²­í•  (ì¢Œí‘œ ìˆëŠ”) ìŒë§Œ ìˆ˜ì§‘í•˜ê³ , ì¢Œí‘œ ì—†ëŠ” ìŒì€ í´ë°±ìœ¼ë¡œ ì²˜ë¦¬
    for start_node, end_node in trip_legs:
        if start_node['id'] == end_node['id']: continue

        ckey = make_cache_key(start_node, end_node, departure_time)
        if ckey in DETAILED_PATH_CACHE:
            path_map[(start_node['id'], end_node['id'])] = DETAILED_PATH_CACHE[ckey]
            continue

        # ì¢Œí‘œê°€ ì—†ìœ¼ë©´ r5 ìš”ì²­ì„ ë§Œë“¤ì§€ ì•Šê³  í´ë°±ìœ¼ë¡œ ì±„ì›€
        if start_node.get('lat') is None or end_node.get('lat') is None:
            fallback = {"fastest": [f"ì´ë™(ì¢Œí‘œì—†ìŒ) : {FALLBACK_MOVE_MIN}ë¶„"], 
                        "min_transfer": [f"ì´ë™(ì¢Œí‘œì—†ìŒ) : {FALLBACK_MOVE_MIN}ë¶„"]}
            path_map[(start_node['id'], end_node['id'])] = fallback
            continue

        # ì¢Œí‘œê°€ ëª¨ë‘ ìˆìœ¼ë©´ r5 ìš”ì²­ ëŒ€ìƒì— ì¶”ê°€
        origins_list.append(start_node)
        dests_list.append(end_node)

    # 2) ì¢Œí‘œ ìˆëŠ” ìŒë§Œ r5pyë¡œ ìƒì„¸ ê²½ë¡œ ìš”ì²­
    if origins_list:
        origins_gdf = gpd.GeoDataFrame(origins_list, geometry=gpd.points_from_xy([n['lng'] for n in origins_list], [n['lat'] for n in origins_list]), crs="EPSG:4326")
        origins_gdf["id"] = [n["id"] for n in origins_list]
        dests_gdf = gpd.GeoDataFrame(dests_list, geometry=gpd.points_from_xy([n['lng'] for n in dests_list], [n['lat'] for n in dests_list]), crs="EPSG:4326")
        dests_gdf["id"] = [n["id"] for n in dests_list]

        try:
            computer = DetailedItineraries(
                transport_network, origins=origins_gdf, destinations=dests_gdf, departure=departure_time,
                transport_modes=[TransportMode.WALK, TransportMode.TRANSIT],
                max_public_transport_rides=MAX_TRANSFERS, max_time=timedelta(minutes=MAX_TRAVEL_TIME_MIN)
            )
        except Exception as e:
            print(f"âš ï¸ DetailedItineraries í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            computer = None

        if computer is not None and not computer.empty:
            mode_col = 'transport_mode' if 'transport_mode' in computer.columns else 'mode'

            def get_val(row, candidates, default=None):
                for c in candidates:
                    if c in row.index and pd.notna(row[c]): return str(row[c]).strip()
                return default

            def parse_route_to_segments(route_df):
                segs = []
                for _, leg in route_df.iterrows():
                    raw_mode = str(leg[mode_col]).upper() if mode_col in leg.index else ''

                    # ì‹œê°„ íŒŒì‹±
                    ride_time = max(1, duration_to_minutes(get_val(leg, ['travel_time', 'duration'], 0)))
                    wait_time = duration_to_minutes(get_val(leg, ['wait_time', 'wait'], 0))

                    # ì¶œë°œ ì •ë¥˜ì¥ ID (ëŒ€ê¸°í•˜ëŠ” ê³³)
                    f_id = str(get_val(leg, ['start_stop_id', 'from_stop_id'])).strip()
                    t_id = str(get_val(leg, ['end_stop_id', 'to_stop_id'])).strip()

                    if wait_time > 0:
                        # [í•µì‹¬ ìˆ˜ì •] ëŒ€ê¸° í…ìŠ¤íŠ¸ ë’¤ì— ì •ë¥˜ì¥ IDë¥¼ ëª°ë˜ ì‹¬ì–´ë‘¡ë‹ˆë‹¤.
                        # ì˜ˆ: "ëŒ€ê¸° : 5ë¶„ [STOP:1000023]"
                        segs.append(f"ëŒ€ê¸° : {wait_time}ë¶„ [STOP:{f_id}]")

                    if 'WALK' in raw_mode:
                        segs.append(f"ë„ë³´ : {ride_time}ë¶„")
                        continue

                    f_stop, t_stop = get_stop_name(f_id) or "ì •ë¥˜ì¥", get_stop_name(t_id) or "ì •ë¥˜ì¥"
                    c_rid = str(get_val(leg, ['route_id']))
                    mode_lbl = "ì§€í•˜ì² " if any(x in raw_mode for x in ['SUBWAY', 'RAIL', 'METRO']) else "ë²„ìŠ¤"

                    # ... (ë²„ìŠ¤ ë…¸ì„ ëª… ì°¾ëŠ” ë¡œì§ ê¸°ì¡´ ë™ì¼) ...
                    if mode_lbl == "ë²„ìŠ¤" and STOP_ROUTE_MAP:
                        common = STOP_ROUTE_MAP.get(f_id, set()).intersection(STOP_ROUTE_MAP.get(t_id, set()))
                        common.add(c_rid)
                        b_names = sorted([n for n in [get_route_name(rid) for rid in common] if n])
                        r_str = ", ".join(b_names) if b_names else (get_route_name(c_rid) or 'ëŒ€ì¤‘êµí†µ')
                    else:
                        r_str = get_route_name(c_rid) or 'ëŒ€ì¤‘êµí†µ'

                    segs.append(f"[{mode_lbl}][{r_str}] : {f_stop} â†’ {t_stop} : {ride_time}ë¶„")

                return segs

            # 3) ê·¸ë£¹ë³„ ì˜µì…˜ ë¶„ì„
            for (from_id, to_id), group in computer.groupby(['from_id', 'to_id']):
                options_data = []
                for _, opt in group.groupby("option"):
                    t_min = sum(max(1, duration_to_minutes(get_val(leg, ['travel_time', 'duration'], 0))) for _, leg in opt.iterrows())
                    t_count = sum(1 for _, leg in opt.iterrows() if 'WALK' not in str(leg[mode_col]).upper())
                    options_data.append({"route": opt, "time": t_min, "transfers": t_count})

                if not options_data:
                    continue

                fastest_opt = min(options_data, key=lambda x: (x['time'], x['transfers']))
                result_entry = {"fastest": parse_route_to_segments(fastest_opt['route'])}

                walk_opts = [o for o in options_data if o['transfers'] == 0]
                best_walk = min(walk_opts, key=lambda x: x['time']) if walk_opts else None

                transit_opts = [o for o in options_data if o['transfers'] > 0]
                transit_opts.sort(key=lambda x: (x['transfers'], x['time']))
                best_transit = transit_opts[0] if transit_opts else None

                winner_opt = None
                if best_walk and best_transit:
                    if best_walk['time'] <= best_transit['time']:
                        winner_opt = best_walk
                    else:
                        winner_opt = best_transit
                elif best_transit:
                    winner_opt = best_transit
                else:
                    winner_opt = best_walk

                if winner_opt:
                    result_entry["min_transfer"] = parse_route_to_segments(winner_opt['route'])
                else:
                    # ë“œë¬¼ê²Œ ì˜µì…˜ì´ ë¹„ì–´ìˆìœ¼ë©´ í´ë°± ì ìš©
                    result_entry["min_transfer"] = [f"ë„ë³´ : {FALLBACK_MOVE_MIN}ë¶„"]

                cache_key = (int(from_id), int(to_id), int(departure_time.hour))
                DETAILED_PATH_CACHE[cache_key] = result_entry
                path_map[(int(from_id), int(to_id))] = result_entry

    return path_map

# ============================================================
# 5. ë…¸ë“œ ë¹Œë” & ìµœì í™” (OR-Tools)
# ============================================================

def build_fixed_nodes(fixed_events, day_start_dt):
    nodes = []
    BUFFER = 15
    for idx, event in enumerate(fixed_events):
        event_start = parse_time(event["start"]) if event.get("start") else day_start_dt
        event_end = parse_time(event["end"]) if event.get("end") else day_start_dt
        orig_start_min = int((event_start - day_start_dt).total_seconds() / 60)
        orig_end_min = int((event_end - day_start_dt).total_seconds() / 60)

        raw_start_min = orig_start_min - BUFFER
        buffered_start_min = max(0, raw_start_min)
        final_stay = (orig_end_min - orig_start_min) + (orig_start_min - buffered_start_min) + BUFFER

        # ê³ ì •ì¼ì •ì€ ì¢Œí‘œê°€ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ lat/lngëŠ” Noneìœ¼ë¡œ ë‘ 
        nodes.append({
            "name": event.get("title", "ê³ ì •ì¼ì •"), "category": "ê³ ì •ì¼ì •", "lat": None, "lng": None,
            "stay": final_stay, "type": "fixed", "window": (buffered_start_min, buffered_start_min + 10),
            "orig_time_str": f"{event.get('start','00:00')} - {event.get('end','00:00')}"
        })
    return nodes

def build_nodes(places, restaurants, fixed_events, day_start_dt):
    nodes = []
    first_place = places[0] if places else {"lat": 37.5665, "lng": 126.9780}
    nodes.append({"name": "ì‹œì‘ì ", "category": "ì¶œë°œ", "lat": first_place["lat"], "lng": first_place["lng"], "stay": 0, "type": "depot"})

    for p in places:
        nodes.append({
            "name": p["name"], 
            "category": p["category"], 
            "category2": p.get("category2", ""), # category2 ì¶”ê°€
            "lat": p.get("lat"), 
            "lng": p.get("lng"), 
            "stay": stay_time_map.get(p["category"], 60), 
            "type": "spot"
        })

    if len(restaurants) >= 2:
        nodes.append({
            "name": restaurants[0]["name"], 
            "category": "ìŒì‹ì ", 
            "category2": restaurants[0].get("category2", "ì‹ë‹¹"), # category2 ì¶”ê°€
            "lat": restaurants[0].get("lat"), 
            "lng": restaurants[0].get("lng"), 
            "stay": 70, 
            "type": "lunch"
        })
        dinner_idx = 1 if restaurants[0]["name"] != restaurants[1]["name"] else 2
        if len(restaurants) > dinner_idx:
            nodes.append({
                "name": restaurants[1]["name"], 
                "category": "ìŒì‹ì ", 
                "category2": restaurants[1].get("category2", "ì‹ë‹¹"), # category2 ì¶”ê°€
                "lat": restaurants[1].get("lat"), 
                "lng": restaurants[1].get("lng"), 
                "stay": 70, 
                "type": "dinner"
            })

    nodes.extend(build_fixed_nodes(fixed_events, day_start_dt))
    return nodes

def build_time_windows(nodes, day_start_dt):
    windows = []
    def get_rel(t_str): return int((parse_time(t_str) - day_start_dt).total_seconds() / 60)
    
    l_s, l_e = get_rel(LUNCH_WINDOW[0]), get_rel(LUNCH_WINDOW[1])
    d_s, d_e = get_rel(DINNER_WINDOW[0]), get_rel(DINNER_WINDOW[1])

    for n in nodes:
        if n["type"] == "lunch": windows.append((l_s, l_e))
        elif n["type"] == "dinner": windows.append((d_s, d_e))
        elif n["type"] == "fixed": windows.append(n["window"])
        else: windows.append((0, 24 * 60))
    return windows

# optimize_day í•¨ìˆ˜ëŠ” ê¸°ì¡´ ë¡œì§ì„ ìœ ì§€í•˜ë˜, get_all_detailed_pathsì—ì„œ ì¢Œí‘œ ì—†ëŠ” êµ¬ê°„ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ë¯€ë¡œ
# ì´í›„ ë¡œì§ì€ ëŒ€ë¶€ë¶„ ê·¸ëŒ€ë¡œ ë™ì‘í•œë‹¤.

def optimize_day(places, restaurants, fixed_events, start_time_str, target_date_str, end_time_str=None):
    TRAVEL_BUFFER = 5
    day_start_dt = datetime.strptime(start_time_str, "%H:%M")
    
    SAFE_GTFS_DATE = target_date_str
    r5_departure_dt = datetime.combine(datetime.strptime(SAFE_GTFS_DATE, "%Y-%m-%d"), datetime.strptime("11:00", "%H:%M").time())
    display_start_dt = datetime.combine(datetime.strptime(target_date_str, "%Y-%m-%d"), day_start_dt.time())

    max_horizon_minutes = 24 * 60
    if end_time_str:
        diff = int((datetime.strptime(end_time_str, "%H:%M") - day_start_dt).total_seconds() / 60)
        if diff > 0: max_horizon_minutes = diff

    nodes = build_nodes(places, restaurants, fixed_events, day_start_dt)
    for idx, node in enumerate(nodes): node["id"] = idx
    n = len(nodes)

    r5_travel_times = get_r5py_matrix(nodes, r5_departure_dt)
    time_matrix = [[0]*n for _ in range(n)]
    
    for i in range(n):
        for j in range(n):
            if i == j: continue
            val = r5_travel_times.get((i, j))
            if val is None: val = travel_minutes(nodes[i], nodes[j])
            
            # ê³ ì •ì¼ì • ì´ë™ì‹œê°„ ë³´ì •
            if (nodes[i]["type"]=="fixed" or nodes[j]["type"]=="fixed"):
                if not (nodes[i]["type"]=="depot" and nodes[j]["type"]=="fixed"):
                    val = max(val, 30)
            
            time_matrix[i][j] = nodes[i]["stay"] + int(val)

    # OR-Tools Solver
    manager = pywrapcp.RoutingIndexManager(n, 1, 0)
    routing = pywrapcp.RoutingModel(manager)

    def time_callback(from_idx, to_idx):
        return time_matrix[manager.IndexToNode(from_idx)][manager.IndexToNode(to_idx)]

    transit_callback = routing.RegisterTransitCallback(time_callback)
    routing.SetArcCostEvaluatorOfAllVehicles(transit_callback)
    routing.AddDimension(transit_callback, 480, max_horizon_minutes, False, "Time")
    time_dim = routing.GetDimensionOrDie("Time")

    time_windows = build_time_windows(nodes, day_start_dt)
    solver = routing.solver()

    for i, node in enumerate(nodes):
        index = manager.NodeToIndex(i)
        if node["type"] == "depot": continue
        
        window = time_windows[i]
        if node["type"] == "fixed":
            time_dim.CumulVar(index).SetRange(max(0, window[0]), min(max_horizon_minutes, window[1]))
            continue

        overlap_start, overlap_end = max(0, window[0]), min(max_horizon_minutes, window[1])
        if overlap_start > overlap_end:
            routing.AddDisjunction([index], 0)
            solver.Add(routing.VehicleVar(index) == -1)
        else:
            time_dim.CumulVar(index).SetRange(overlap_start, overlap_end)
            penalty = 1000000 if node["type"] in ["lunch", "dinner"] else 100000
            routing.AddDisjunction([index], penalty)

    search_params = pywrapcp.DefaultRoutingSearchParameters()
    # search_params.local_search_metaheuristic = routing_enums_pb2.LocalSearchMetaheuristic.GUIDED_LOCAL_SEARCH
    # search_params.time_limit.seconds = 1
    # search_params.log_search = False

    solution = routing.SolveWithParameters(search_params)
    if not solution: return []

    index = routing.Start(0)
    visited_nodes = []
    while not routing.IsEnd(index):
        node_idx = manager.IndexToNode(index)
        nodes[node_idx]['arrival_min'] = solution.Value(time_dim.CumulVar(index))
        visited_nodes.append(nodes[node_idx])
        index = solution.Value(routing.NextVar(index))

    trip_legs = [(visited_nodes[i], visited_nodes[i+1]) for i in range(len(visited_nodes)-1)]
    
    print("ğŸš€ ìƒì„¸ ê²½ë¡œ ê³„ì‚° ì¤‘...")
    start_path_time = time.time()
    path_map = get_all_detailed_paths(trip_legs, r5_departure_dt)
    end_path_time = time.time()
    print(f"â± ìƒì„¸ ê²½ë¡œ ê³„ì‚° ì™„ë£Œ: {round(end_path_time - start_path_time, 2)}ì´ˆ")

    def build_timeline_by_type(path_type):
        timeline = []
        actual_visits = [n for n in visited_nodes if n["type"] != "depot"]
        
        # ì²« ë²ˆì§¸ ì¥ì†Œ ë„ì°© ì‹œê°„ ê¸°ì¤€
        cursor_dt = display_start_dt + timedelta(minutes=actual_visits[0]['arrival_min'])

        for i, node in enumerate(actual_visits):
            transit_info = []
            travel_min = 0
            
            # ============================================================
            # 1. ì´ë™ ê²½ë¡œ ë° ëŒ€ê¸° ì‹œê°„ ê³„ì‚° (ì •ë¥˜ì¥ í˜¼ì¡ë„ ë°˜ì˜)
            # ============================================================
            if i > 0:
                prev = actual_visits[i-1]
                path_options = path_map.get((prev['id'], node['id']))
                if path_options:
                    chosen_path = path_options.get(path_type, path_options.get('fastest', []))
                    
                    for segment in chosen_path:
                        seg_mins = sum(int(m) for m in re.findall(r'(\d+)ë¶„', segment))
                        
                        if "ëŒ€ê¸°" in segment:
                            target_lat, target_lng = None, None
                            
                            stop_match = re.search(r'\[STOP:(.*?)\]', segment)
                            if stop_match:
                                s_id = stop_match.group(1).strip()
                                if s_id in STOP_COORDS:
                                    target_lat = STOP_COORDS[s_id]['lat']
                                    target_lng = STOP_COORDS[s_id]['lng']
                            
                            if target_lat is None:
                                target_lat = prev.get('lat')
                                target_lng = prev.get('lng')

                            cong_level = get_congestion_level(target_lat, target_lng, cursor_dt)
                            weight = get_wait_weight(cong_level) # ëŒ€ê¸° ì‹œê°„ ê°€ì¤‘ì¹˜ ì‚¬ìš©
                            
                            weighted_wait = int(seg_mins * weight)
                            added_wait = weighted_wait - seg_mins
                            
                            icons = {0: "ğŸŸ¢", 1: "ğŸŸ¡", 2: "ğŸ”´"}
                            cong_icon = icons.get(cong_level, "")

                            clean_segment = re.sub(r'\s*\[STOP:.*?\]', '', segment) 
                            clean_segment += f" {cong_icon}"
                            
                            if added_wait > 0:
                                seg_mins = weighted_wait
                                clean_segment += f"(+{added_wait}ë¶„)"
                            
                            segment = clean_segment
                                
                        transit_info.append(segment)
                        travel_min += seg_mins
                else:
                    dist = haversine(prev['lat'], prev['lng'], node['lat'], node['lng']) if prev.get('lat') else 0
                    travel_min = int(dist * 15) if dist > 0 else FALLBACK_MOVE_MIN
                    transit_info.append(f"ë„ë³´ : {travel_min}ë¶„")

            # ============================================================
            # 2. ë„ì°© ì‹œê°„ í™•ì • (ì´ë™ ì‹œê°„ ë°˜ì˜)
            # ============================================================
            arrival_dt = cursor_dt + timedelta(minutes=travel_min)
            
            # ì‹ì‚¬ ì‹œê°„ ìœˆë„ìš° ì²´í¬ (ë„ˆë¬´ ì¼ì° ë„ì°©í•˜ë©´ ëŒ€ê¸°)
            if node["type"] in ["lunch", "dinner"]:
                window_start_min, _ = build_time_windows([node], display_start_dt)[0]
                window_start_dt = display_start_dt + timedelta(minutes=window_start_min)
                earliest_start_dt = window_start_dt - timedelta(minutes=20) # 20ë¶„ ì „ê¹Œì§„ í—ˆìš©
                
                if arrival_dt < earliest_start_dt:
                    wait_min = int((window_start_dt - arrival_dt).total_seconds() / 60)
                    transit_info.append(f"í˜„ì¥ ëŒ€ê¸° : {wait_min}ë¶„")
                    arrival_dt = window_start_dt

            # ============================================================
            # 3. [í•µì‹¬] ì²´ë¥˜ ì‹œê°„ ê³„ì‚° (í˜¼ì¡ë„ ê°€ì¤‘ì¹˜ ì ìš©)
            # ============================================================
            final_stay_min = node["stay"]
            congestion_label = ""
            
            # (A) ê³ ì • ì¼ì • ë° ì¶œë°œì§€ê°€ ì•„ë‹Œ ê²½ìš° í˜¼ì¡ë„ ê³„ì‚°
            if node["type"] not in ["fixed", "depot"]:
                cong_level = get_congestion_level(node.get('lat'), node.get('lng'), arrival_dt)
                
                labels = {0: "ğŸŸ¢ì—¬ìœ ", 1: "ğŸŸ¡ë³´í†µ", 2: "ğŸ”´í˜¼ì¡"}
                congestion_label = labels.get(cong_level, "ì •ë³´ì—†ìŒ")
                
                # (B) ëª¨ë“  ì¥ì†ŒëŠ” ì²´ë¥˜ ì‹œê°„ ëŠ˜ë¦¬ê¸°
                weight = get_stay_weight(cong_level) # ì²´ë¥˜ ì‹œê°„ ê°€ì¤‘ì¹˜ ì‚¬ìš©
                
                original_stay = node["stay"]
                final_stay_min = int(original_stay * weight)
                
                # ì‹œê°„ì´ ëŠ˜ì–´ë‚¬ìœ¼ë©´ ë¡œê·¸(ë””ë²„ê¹…ìš©) í˜¹ì€ ê²°ê³¼ì— í‘œì‹œí•  ìˆ˜ë„ ìˆìŒ
                if final_stay_min > original_stay:
                    # ì˜ˆ: "ğŸ”´í˜¼ì¡(+18ë¶„)" ì²˜ëŸ¼ í‘œì‹œí•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
                    # congestion_label += f"(+{final_stay_min - original_stay}ë¶„)"
                    pass

            elif node["type"] == "fixed":
                congestion_label = "ğŸ“…ê³ ì •"

            # ============================================================
            # 4. ì¢…ë£Œ ì‹œê°„ ê³„ì‚° ë° ì»¤ì„œ ì—…ë°ì´íŠ¸
            # ============================================================
            if node["type"] == "fixed":
                time_str = node.get("orig_time_str", "00:00 - 00:00")
                time_parts = time_str.split(" - ")
                # ê³ ì • ì¼ì •ì€ ì •í•´ì§„ ì‹œê°„ì— ëë‚˜ë¯€ë¡œ ì»¤ì„œë¥¼ ê°•ì œë¡œ ë§ì¶¤
                cursor_dt = datetime.strptime(f"{target_date_str} {time_parts[1]}", "%Y-%m-%d %H:%M")
            else:
                # ì¼ë°˜ ì¥ì†ŒëŠ” ëŠ˜ì–´ë‚œ ì²´ë¥˜ì‹œê°„(final_stay_min)ë§Œí¼ ë¨¸ë¬¼ê³  ì¶œë°œ
                end_dt = arrival_dt + timedelta(minutes=final_stay_min)
                time_str = f"{arrival_dt.strftime('%H:%M')} - {end_dt.strftime('%H:%M')}"
                cursor_dt = end_dt

            # ============================================================
            # 5. ê²°ê³¼ ì €ì¥
            # ============================================================
            timeline.append({
                "name": node['name'], 
                "category": node["category"],
                "category2": node.get("category2", node["category"]),
                "time": time_str,
                "transit_to_here": transit_info,
                "congestion_level": congestion_label
            })
        return timeline

    return {
        "fastest_version": build_timeline_by_type("fastest"),
        "min_transfer_version": build_timeline_by_type("min_transfer")
    }

# ============================================================
# 6. ë©”ì¸ ì‹¤í–‰ë¶€ (í†µí•©)
# ============================================================
if __name__ == "__main__":
    # 1. ì—‘ì…€ ë° ê¸°ë³¸ ì •ë³´ ë¡œë“œ
    print("ğŸ“‚ ì¥ì†Œ ë°ì´í„° ë¡œë“œ ì¤‘...")
    try:
        df = pd.read_excel("./data/place_ì „ì²´_í†µí•©_ì§„ì§œìµœì¢….xlsx")
    except FileNotFoundError:
        print("âŒ 'places_ì „ì²´_í†µí•©.xlsx' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    area = input("ì—¬í–‰í•  ì§€ì—­ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì¢…ë¡œêµ¬): ")

    if area not in SEOUL_GU_COORDS:
        raise ValueError("ì„œìš¸ êµ¬ ì´ë¦„ì´ ì•„ë‹™ë‹ˆë‹¤.")
    
    center_lat = SEOUL_GU_COORDS[area]["lat"]
    center_lon = SEOUL_GU_COORDS[area]["lon"]

    df["distance_km"] = df.apply(lambda r: haversine(center_lat, center_lon, r["lat"], r["lng"]), axis=1)
    RADIUS_KM = 6
    
    # 2. ì¥ì†Œ í•„í„°ë§
    area_mask = df[df["distance_km"] <= RADIUS_KM].copy()
    print(f"\nğŸ“ {area} ì¤‘ì‹¬ ë°˜ê²½ {RADIUS_KM}km ì´ë‚´ ì¥ì†Œ ìˆ˜: {len(area_mask)}")

    dist_mask = df["distance_km"] <= RADIUS_KM

    filtered_spot = df[dist_mask & (df["category"] != "ìŒì‹ì ") & (df["category"] != "ìˆ™ë°•")][["name", "lat", "lng", "category", "category2"]]

    avg_lat = filtered_spot["lat"].mean()
    avg_lng = filtered_spot["lng"].mean()

    # ê´€ê´‘ì§€ ì¤‘ì‹¬ 1.5km ì´ë‚´ ì‹ë‹¹ë§Œ ì¶”ì¶œ (í›¨ì”¬ íƒ€ì´íŠ¸í•œ ë™ì„ )
    df["dist_to_center"] = df.apply(lambda r: haversine(avg_lat, avg_lng, r["lat"], r["lng"]), axis=1)
    filtered_restaurant = df[(df["dist_to_center"] <= 3) & (df["category"] == "ìŒì‹ì ")][["name", "lat", "lng", "category", "category2"]]

    filtered_accom = df[dist_mask & (df["category"] == "ìˆ™ë°•")][["name", "lat", "lng", "category", "category2"]]

    places = filtered_spot.to_dict(orient="records")
    print(len(places), "ê°œì˜ ê´€ê´‘ì§€ê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")

    restaurants = filtered_restaurant.to_dict(orient="records")
    print(len(restaurants), "ê°œì˜ ìŒì‹ì ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")

    accommodations = filtered_accom.to_dict(orient="records")
    print(len(accommodations), "ê°œì˜ ìˆ™ë°• ì‹œì„¤ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 3. ë‚ ì§œ ì…ë ¥
    start_date = input("ì—¬í–‰ ì‹œì‘ ì¼ì (ì˜ˆ: 2026-01-20): ")
    end_date = input("ì—¬í–‰ ì¢…ë£Œ ì¼ì (ì˜ˆ: 2026-01-25): ")
    
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")
    days = (end - start).days + 1
    print(f"ì´ ì—¬í–‰ ì¼ìˆ˜: {days}ì¼")

    # # 4. Gemini API í˜¸ì¶œ (1ì°¨ ê³„íš ìƒì„±)
    # schema = """
    # {
    #   "plans": {
    #     "day1": {
    #       "route": [
    #         {"name": "...", "category": "...", "category2": "...", "lat": 0.0, "lng": 0.0}
    #       ],
    #       "restaurants": [
    #         {"name": "...", "category": "...", "category2": "...", "lat": 0.0, "lng": 0.0}
    #       ],
    #       "accommodations": [
    #         {"name": "...", "category": "...", "category2": "...", "lat": 0.0, "lng": 0.0}
    #       ]
    #     }
    #   }
    # }
    # """
    
    # system_prompt = f"""
    # ë„ˆëŠ” 'ì„œìš¸ ì—¬í–‰ ì¥ì†Œ ì¶”ì²œ ì „ë¬¸ê°€'ì´ë‹¤. ë°˜ë“œì‹œ ì œê³µëœ ë°ì´í„°ë§Œì„ ì‚¬ìš©í•˜ì—¬ ê³„íšì„ ì„¸ìš´ë‹¤.
    # {schema}
    # [ì ˆëŒ€ ê·œì¹™]
    # 1. ëª¨ë“  ì¥ì†Œì˜ ì´ë¦„, ì¢Œí‘œ(lat, lng), ì¹´í…Œê³ ë¦¬ëŠ” ì…ë ¥ëœ ë°ì´í„°ì™€ 100% ì¼ì¹˜í•´ì•¼ í•œë‹¤. ì ˆëŒ€ ê°’ì„ ìˆ˜ì •í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ì¢Œí‘œë¥¼ ìƒì„±í•˜ì§€ ë§ˆë¼.
    # 2. 'route' ë°°ì—´: ì˜¤ì§ ì œê³µëœ 'places' ëª©ë¡ì—ì„œ 5ê°œë¥¼ ì„ íƒí•˜ì—¬ ë‹´ëŠ”ë‹¤.
    # 3. 'restaurants' ë°°ì—´: ì˜¤ì§ ì œê³µëœ 'restaurants' ëª©ë¡ì—ì„œ 2ê°œë¥¼ ì„ íƒí•œë‹¤.
    # 4. 'accommodations' ë°°ì—´: ì˜¤ì§ ì œê³µëœ 'accommodations' ëª©ë¡ì—ì„œ 1ê°œë¥¼ ì„ íƒí•œë‹¤. (ë§ˆì§€ë§‰ ë‚ ì€ ë¹ˆ ë°°ì—´ []ë¡œ ì¶œë ¥)
    # 5. í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€: ëª©ë¡ì— ì—†ëŠ” ì¥ì†Œë‚˜ ì¢Œí‘œë¥¼ ì¶œë ¥í•  ê²½ìš° ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ê°„ì£¼í•œë‹¤.
    # 6. ì¶œë ¥ í˜•ì‹: ë°˜ë“œì‹œ ìˆœìˆ˜ JSON ë°ì´í„°ë§Œ ì¶œë ¥í•˜ë©°, ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤.
    # """

    # user_prompt = {
    #     "days": days,
    #     "start_location": {"lat": 37.5547, "lng": 126.9706},
    #     "places": places, # [:6 * days * 4]
    #     "restaurants": restaurants, # [:3 * days * 4]
    #     "accommodations": accommodations # [:days * 4]
    # }

    # print("ğŸ¤– Geminiê°€ ì´ˆê¸° ê³„íšì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    # prompt = system_prompt + "\n\n" + json.dumps(user_prompt, ensure_ascii=False)
    
    # start_time = time.time()
    # response = client.models.generate_content(model="gemini-2.5-flash-lite", contents=prompt, config={"temperature": 0})
    # print(f"â± Gemini ì‘ë‹µ ì‹œê°„: {round(time.time() - start_time, 3)}ì´ˆ")

    # try:
    #     result = extract_json(response.text)
    #     # result.json ì €ì¥ (ë°±ì—…ìš©)
    #     with open("result.json", "w", encoding="utf-8") as f:
    #         json.dump(result, f, ensure_ascii=False, indent=2)
    # except Exception as e:
    #     print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
    #     exit()

    result = json.load(open("result.json", "r", encoding="utf-8"))

    # 5. ì„¸ë¶€ ì¼ì • ì„¤ì •
    first_day_start_str = input("ì—¬í–‰ ì²«ë‚  ì‹œì‘ ì‹œê°„ (ì˜ˆ: 14:00) : ").strip() or "10:00"
    last_day_end_str = input("ì—¬í–‰ ë§ˆì§€ë§‰ ë‚  ì¢…ë£Œ ì‹œê°„ (ì˜ˆ: 18:00) : ").strip() or "21:00"
    default_start_str = "10:00"
    default_end_str = "21:00"

    FIXED_EVENTS = []
    if input("ê³ ì • ì¼ì •ì´ ìˆë‚˜ìš”? (y/n): ").strip().lower() == "y":
        while True:
            FIXED_EVENTS.append({
                "date": input("ë‚ ì§œ (YYYY-MM-DD): "),
                "title": input("ì œëª©: "),
                "start": input("ì‹œì‘(HH:MM): "),
                "end": input("ì¢…ë£Œ(HH:MM): ")
            })
            if input("ë” ì¶”ê°€í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() != "y": break

    # 6. ìµœì í™” ì‹¤í–‰ (Day loop)
    plans = result["plans"]
    day_keys = list(plans.keys())

    print(f"\nğŸš€ ë³‘ë ¬ ìµœì í™” ì‹œì‘: {len(day_keys)}ì¼ì¹˜ ì¼ì •ì„ ë™ì‹œì— ê³„ì‚°í•©ë‹ˆë‹¤.")
    start_total_opt = time.time()

    # 6-1. ë³‘ë ¬ ì‹¤í–‰ ì¸ì(Task) ì¤€ë¹„
    def process_day_wrapper(args):
        day_key, date_obj, is_first, is_last = args
        
        todays_start = first_day_start_str if is_first else default_start_str
        todays_end = last_day_end_str if is_last else default_end_str
        current_date_str = date_obj.strftime("%Y-%m-%d")
        
        print(f"   â–¶ {day_key} ìµœì í™” ì‹œì‘...")
        
        # ì‹¤ì œ ìµœì í™” ìˆ˜í–‰
        day_res = optimize_day(
            places=plans[day_key]["route"],
            restaurants=plans[day_key]["restaurants"],
            fixed_events=get_fixed_events_for_day(FIXED_EVENTS, current_date_str),
            start_time_str=todays_start,
            target_date_str=current_date_str,
            end_time_str=todays_end
        )
        return day_key, day_res

    tasks = []
    curr = start
    for i, day_key in enumerate(day_keys):
        tasks.append((day_key, curr, i==0, i==len(day_keys)-1))
        curr += timedelta(days=1)

    # 6-2. ThreadPoolExecutorë¡œ ë³‘ë ¬ ì‹¤í–‰
    processed_results = {}

    max_workers = min(days, 4)
    print(f"âš™ï¸ ìµœëŒ€ {max_workers}ê°œ ì½”ì–´ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ì¤‘...")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        results = list(executor.map(process_day_wrapper, tasks))
        for day_key, day_res in results:
            processed_results[day_key] = day_res
            print(f"   âœ… {day_key} ì™„ë£Œ")

    print(f"â± ì „ì²´ ìµœì í™” ì™„ë£Œ: {round(time.time() - start_total_opt, 2)}ì´ˆ")
    
    # 3. ê²°ê³¼ ì·¨í•© ë° í™”ë©´ ì¶œë ¥
    curr = start
    for i, day_key in enumerate(day_keys):
        # ê²°ê³¼ ì €ì¥
        result["plans"][day_key]["timelines"] = processed_results[day_key]
        day_results = processed_results[day_key]
        
        print(f"\nğŸ“… {day_key} ({curr.strftime('%Y-%m-%d')})")

        # ë‘ ê°€ì§€ ë²„ì „(ìµœë‹¨ ì‹œê°„, ìµœì†Œ í™˜ìŠ¹) ëª¨ë‘ ì¶œë ¥
        for ver_key, label in [("fastest_version", "ìµœë‹¨ ì‹œê°„"), ("min_transfer_version", "ìµœì†Œ í™˜ìŠ¹")]:
            timeline = day_results[ver_key]
            
            separator = "-" * 60
            print(f"\n[{label} ê¸°ì¤€ ì¼ì •] {day_key}")
            print(separator)

            for t in timeline:
                if t.get('transit_to_here'):
                    path_str = " -> ".join([s for s in t['transit_to_here']])
                    print(f"  [TRANSIT] {path_str}")
                
                # category ëŒ€ì‹  category2 ì¶œë ¥
                display_cat = t.get('category2', t['category'])
                
                # [ìˆ˜ì •ëœ ì¶œë ¥ í¬ë§·]
                # ê¸°ì¡´: [{t['time']}] {t['name']} ({display_cat})
                # ë³€ê²½: [{t['time']}] {t['name']} ({display_cat}) {t['congestion_level']}
                print(f"  [{t['time']}] {t['name']} ({display_cat}) {t['congestion_level']}")
            
            print(separator)

        # ë‚ ì§œ ì¹´ìš´í„° ì¦ê°€
        curr += timedelta(days=1)

    # 7. ëª¨ë“  ë£¨í”„ê°€ ëë‚œ í›„ ìµœì¢… íŒŒì¼ ì €ì¥ (ë£¨í”„ ì™¸ë¶€)
    with open("result_timeline.json", "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
        print("\nì „ì²´ ì¼ì •ì´ 'result_timeline.json' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")